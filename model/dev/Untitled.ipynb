{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "\n",
    "# load the trained model\n",
    "sf = \"/media/kiran/D0-P1/animal_data/meerkat/EXAMPLE_NoiseAugmented_0.3_0.8_NotWeighted_MaskedOther_Forked/trained_model/EXAMPLE_NoiseAugmented_0.3_0.8_NotWeighted_MaskedOther_Forked_2021-05-12_00:19:17.080719\"\n",
    "RNN_model = load_model(sf + '/savedmodel' + '.h5') \n",
    "\n",
    "\n",
    "labelling_logpath = \"/home/kiran/Dropbox/CCAS_big_data/meerkat_labelling_times.csv\"\n",
    "labelling_log = pd.read_csv(labelling_logpath,  sep = \",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelling_log = labelling_log.loc[labelling_log[\"status\"]== \"completed\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_filenames = list(labelling_log[\"full_file_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset all the audio files that we should use in the analysis (i.e. not focal follow data)\n",
    "audio_files = list(set(labels_all[\"wavFileName\"]))\n",
    "audio_filenames = list(compress(audio_files, [\"SOUNDFOC\" not in filei for filei in audio_files]))\n",
    "\n",
    "# subset all the audio files that we should use in the analysis (i.e. not focal follow data)\n",
    "label_files = list(set(labels_all[\"csvFileName\"]))\n",
    "label_filenames = list(compress(label_files, [\"SOUNDFOC\" not in filei for filei in label_files]))\n",
    "\n",
    "# get the file IDS without all the extentions (used later for naming)\n",
    "all_filenames = [audio_filenames[i].split(\".\")[0] for i in range(0,len(audio_filenames))]\n",
    "\n",
    "# find all the labels\n",
    "EXT = \"*.csv\"\n",
    "label_filepaths = []\n",
    "for PATH in acoustic_data_path :\n",
    "      label_filepaths.extend( [file for path, subdir, files in os.walk(PATH) for file in glob.glob(os.path.join(path, EXT))])\n",
    "EXT = \"*.CSV\"\n",
    "for PATH in acoustic_data_path :\n",
    "      label_filepaths.extend( [file for path, subdir, files in os.walk(PATH) for file in glob.glob(os.path.join(path, EXT))])\n",
    "\n",
    "# find all audio paths (will be longer than label path as not everything is labelled)\n",
    "audio_filepaths = []\n",
    "EXT = \"*.wav\"\n",
    "for PATH in audio_dirs:\n",
    "      audio_filepaths.extend( [file for path, subdir, files in os.walk(PATH) for file in glob.glob(os.path.join(path, EXT))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_table[\"wav_path\"] = label_table['wavFileName'].apply(lambda x: [pathi for pathi in audio_filepaths if x in pathi][0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_ID = testing_filenames[0]\n",
    "# subset the label table to only use that one file\n",
    "label_table = testing_label_table[testing_label_table['wavFileName'].isin([file_ID])].reset_index()\n",
    "\n",
    "# Find the file ID name i.e. remove the extention\n",
    "file_ID = file_ID.split(\".\")[0] \n",
    "\n",
    "# find the matching audio for the label data\n",
    "audio_path = label_table[\"wav_path\"][0]  \n",
    "\n",
    "\n",
    "skipped_files = []\n",
    "\n",
    "# we only do one file in this example to avoid it running too long\n",
    "for file_ID in testing_filenames:\n",
    "    # file_ID = testing_filenames[0] # for testing purposes only\n",
    "    \n",
    "    # subset the label table to only use that one file\n",
    "    label_table = testing_label_table[testing_label_table['wavFileName'].isin([file_ID])].reset_index()\n",
    "    # Find the file ID name i.e. remove the extention\n",
    "    file_ID = file_ID.split(\".\")[0] \n",
    "    # find the matching audio for the label data\n",
    "    audio_path = label_table[\"wav_path\"][0]   \n",
    "    \n",
    "    print(\"*****************************************************************\")   \n",
    "    print(\"*****************************************************************\") \n",
    "    print (\"File being processed : \" + audio_path)    \n",
    "    \n",
    "    # find the start and stop  of the labelling periods (also using skipon/skipoff)\n",
    "    loop_table = label_table.loc[label_table[\"Label\"].str.contains('|'.join(label_for_startstop), regex=True, case = False), [\"Label\",\"Start\"]]\n",
    "    loop_times = list(loop_table[\"Start\"])\n",
    "    \n",
    "    # Make sure that the file contains the right number of start and stops, otherwise go to the next file\n",
    "    if len(loop_times)%2 != 0:\n",
    "        print(\"!!!!!!!!!!!!!!!!\")\n",
    "        warnings.warn(\"There is a missing start or stop in this file and it has been skipped: \" + audio_path)\n",
    "        skipped_files.append(file_ID)\n",
    "        continue \n",
    "    \n",
    "    if len(loop_times) == 0:\n",
    "        print(\"!!!!!!!!!!!!!!!!\")\n",
    "        warnings.warn(\"There is a missing start or stop in this file and it has been skipped: \" + audio_path)\n",
    "        skipped_files.append(file_ID)\n",
    "        continue \n",
    "    \n",
    "    # save the label_table\n",
    "    save_label_table_filename = file_ID + \"_LABEL_TABLE.txt\"\n",
    "    label_table.to_csv(os.path.join(save_label_table_test_path, save_label_table_filename), \n",
    "                        header=True, index=None, sep=';')\n",
    "    \n",
    "    # load the audio data\n",
    "    y, sr = librosa.load(audio_path, sr=None, mono=False)\n",
    "    \n",
    "    # # Reshaping the Audio file (mono) to deal with all wav files similarly\n",
    "    # if y.ndim == 1:\n",
    "    #     y = y.reshape(1, -1)\n",
    "    \n",
    "    # # Implement this for acc data\n",
    "    # for ch in range(y.shape[0]):\n",
    "    # ch=0\n",
    "    # y_sub = y[:,ch]\n",
    "    y_sub = y\n",
    "    \n",
    "    # loop through every labelling start based on skipon/off within this loop_table\n",
    "    for loopi in range(0, int(len(loop_times)), 2):\n",
    "        # loopi = 0\n",
    "        fromi =  loop_times[loopi]\n",
    "        #toi = fromi + 5\n",
    "        toi = loop_times[int(loopi + 1)] # define the end of the labelling periods\n",
    "        \n",
    "        # if the file exists, load it\n",
    "        if os.path.exists(os.path.join(save_pred_stack_test_path, file_ID + '_CALLTYPE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy')): \n",
    "            calltype_pred_list = np.load( os.path.join(save_pred_stack_test_path, file_ID + '_CALLTYPE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy')) \n",
    "            callpresence_pred_list = np.load( os.path.join(save_pred_stack_test_path, file_ID + '_CALLPRESENCE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy'))\n",
    "       \n",
    "        # if not, generate it\n",
    "        else:\n",
    "        \n",
    "            calltype_pred_list = []\n",
    "            callpresence_pred_list = []\n",
    "    \n",
    "            for spectro_slide in np.arange(fromi, toi, slide):\n",
    "                \n",
    "                # spectro_slide = fromiv\n",
    "                start = round(spectro_slide,3)\n",
    "                stop = round(spectro_slide + spec_window_size, 3)\n",
    "                \n",
    "                # ignore cases where the window is larger than what is labelled (e.g. at the end)\n",
    "                if stop <= toi:\n",
    "                    \n",
    "                    #generate the spectrogram\n",
    "                    spectro = pre.generate_mel_spectrogram(y=y_sub, sr=sr, start=start, stop=stop, \n",
    "                                                            n_mels = n_mels, window='hann', \n",
    "                                                            fft_win= fft_win, fft_hop = fft_hop, \n",
    "                                                            normalise = True)\n",
    "                    \n",
    "                    # transpose it and put it in a format that works with the NN\n",
    "                    spec = spectro.T\n",
    "                    spec = spec[np.newaxis, ..., np.newaxis]  \n",
    "                    \n",
    "                    # generate a mask (as a placeholder) but don't mask anything as we are predicting and want to include other\n",
    "                    mask = np.asarray([True for i in range(spectro.shape[1])])\n",
    "                    mask = mask[np.newaxis,...]\n",
    "                    \n",
    "                    # generate the prediction\n",
    "                    pred = RNN_model.predict([spec,mask])\n",
    "                                       \n",
    "                    # add this prediction to the stack that will be used to generate the predictions table\n",
    "                    if is_forked:\n",
    "                        calltype_pred_list.append(np.squeeze(pred[0]))\n",
    "                        callpresence_pred_list.append(np.squeeze(pred[1]))\n",
    "                    else:\n",
    "                        calltype_pred_list.append(np.squeeze(pred))\n",
    "                    \n",
    "            # save the prediction stacks\n",
    "            np.save( os.path.join(save_pred_stack_test_path, file_ID + '_CALLTYPE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy'), calltype_pred_list)\n",
    "            with open(os.path.join(save_pred_stack_test_path, file_ID + '_CALLTYPE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.txt'), \"w\") as f:\n",
    "                for row in calltype_pred_list:\n",
    "                    f.write(str(row) +\"\\n\")\n",
    "            if is_forked:      \n",
    "                np.save( os.path.join(save_pred_stack_test_path, file_ID + '_CALLPRESENCE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy'), callpresence_pred_list)\n",
    "                with open(os.path.join(save_pred_stack_test_path, file_ID + '_CALLPRESENCE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.txt'), \"w\") as f:\n",
    "                    for row in callpresence_pred_list:\n",
    "                        f.write(str(row) +\"\\n\")\n",
    "                \n",
    "        # Loop through different sets of thresholds\n",
    "        for low_thr in [0.1,0.2, 0.3]:\n",
    "            for high_thr in [0.3,0.4,0.5,0.6,0.5,0.7,0.8,0.9,0.95]: \n",
    "                \n",
    "                # make sure it doesnt generate a 0.00098982374957839486 type number\n",
    "                low_thr = round(low_thr,2)                               \n",
    "                high_thr = round(high_thr,2)\n",
    "                \n",
    "                # stop the loop if the low threshold is bigger than the high threshold\n",
    "                if low_thr >= high_thr:\n",
    "                    continue\n",
    "\n",
    "                save_pred_table_filename = file_ID + \"_CALLTYPE_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + \".txt\"\n",
    "                \n",
    "                # if the file exists, pass to the next iteration of the loop\n",
    "                #if os.path.exists(os.path.join(save_pred_table_test_path, save_pred_table_filename)):\n",
    "                #    continue\n",
    "                \n",
    "                print(\"*****************************************************************\") \n",
    "                print (\"Low Threshold: \" + str(low_thr))    \n",
    "                print (\"High Threshold: \" + str(high_thr))  \n",
    "                \n",
    "                #----------------------------------------------------------------------------\n",
    "                # Compile the predictions for each on/off labelling chunk\n",
    "                detections = ppm.merge_p(probabilities = calltype_pred_list, \n",
    "                                          labels=list(call_types.keys()),\n",
    "                                          starttime = 0, \n",
    "                                          frameadv_s = fft_hop, \n",
    "                                          specadv_s = slide,\n",
    "                                          low_thr=low_thr, \n",
    "                                          high_thr=high_thr, \n",
    "                                          debug=1)\n",
    "                \n",
    "                #in case the dataset was just noise, still create an empty placeholder to merge\n",
    "                if len(detections) == 0:  \n",
    "                    detections = pd.DataFrame(columns = ['category', 'start', 'end', 'scores'])\n",
    "                \n",
    "                # create an empty dataset\n",
    "                pred_table = pd.DataFrame() \n",
    "                \n",
    "                #convert these detections to a predictions table                \n",
    "                table = pd.DataFrame(detections)\n",
    "                table[\"Label\"] = table[\"category\"]\n",
    "                table[\"Start\"] = round(table[\"start\"]*fft_hop + fromi, 3) #table[\"start\"].apply(Decimal)*Decimal(fft_hop) + Decimal(fromi)\n",
    "                table[\"Duration\"] = round( (table[\"end\"]-table[\"start\"])*fft_hop, 3) #(table[\"end\"].apply(Decimal)-table[\"start\"].apply(Decimal))*Decimal(fft_hop)\n",
    "                table[\"End\"] = round(table[\"end\"]*fft_hop + fromi, 3) #table[\"Start\"].apply(Decimal) + table[\"Duration\"].apply(Decimal)\n",
    "                \n",
    "                # keep only the useful columns    \n",
    "                table = table[[\"Label\",\"Start\",\"Duration\", \"End\", \"scores\"]]  \n",
    "                \n",
    "                # Add a row which stores the start of the labelling period\n",
    "                row_start = pd.DataFrame()\n",
    "                row_start.loc[0,'Label'] = list(loop_table[\"Label\"])[loopi]\n",
    "                row_start.loc[0,'Start'] = fromi\n",
    "                row_start.loc[0,'Duration'] = 0\n",
    "                row_start.loc[0,'End'] = fromi \n",
    "                row_start.loc[0,'scores'] = None\n",
    "                \n",
    "                # Add a row which stores the end of the labelling period\n",
    "                row_stop = pd.DataFrame()\n",
    "                row_stop.loc[0,'Label'] = list(loop_table[\"Label\"])[int(loopi + 1)]\n",
    "                row_stop.loc[0,'Start'] = toi\n",
    "                row_stop.loc[0,'Duration'] = 0\n",
    "                row_stop.loc[0,'End'] = toi \n",
    "                row_start.loc[0,'scores'] = None\n",
    "                \n",
    "                # put these rows to the label table\n",
    "                table = pd.concat([row_start, table, row_stop]) \n",
    "                \n",
    "                # add the true false columns based on the call types dictionary\n",
    "                for true_label in call_types:\n",
    "                    table[true_label] = False\n",
    "                    for old_label in call_types[true_label]:\n",
    "                        table.loc[table[\"Label\"].str.contains(old_label, regex=True, case = False), true_label] = True\n",
    "                \n",
    "                # add this table to the overall predictions table for that collar\n",
    "                pred_table = pd.concat([pred_table, table ])\n",
    "                \n",
    "                # for each on/off labelling chunk, we can save the prediction and append it to the previous chunk\n",
    "                if loopi == 0:                    \n",
    "                    # for the first chunck keep the header, but not when appending later\n",
    "                    pred_table.to_csv(os.path.join(save_pred_table_test_path, save_pred_table_filename), \n",
    "                                      header=True, index=None, sep=';', mode = 'a')\n",
    "                else:\n",
    "                    pred_table.to_csv(os.path.join(save_pred_table_test_path, save_pred_table_filename), \n",
    "                                      header=None, index=None, sep=';', mode = 'a')\n",
    " \n",
    "'''\n",
    "# # load the saved file\n",
    "# with open(os.path.join(save_pred_stack_test_path, file_ID + '_PRED_STACK.txt')) as f:\n",
    "#     content = f.readlines()\n",
    "# # remove whitespace characters like `\\n` at the end of each line\n",
    "# pred_list = [x.strip() for x in content] \n",
    "\n",
    "\n",
    "# #or\n",
    "# pred_list = np.load( os.path.join(save_pred_stack_test_path, file_ID + '_PRED_STACK.npy'))\n",
    "# '''\n",
    "        \n",
    "# save the files that were skipped\n",
    "print(skipped_files)\n",
    "\n",
    "# save a copy of the training and testing diles\n",
    "with open(os.path.join(save_model_path, \"skipped_testing_files.txt\"), \"w\") as f:\n",
    "    for s in skipped_files:\n",
    "        f.write(str(s) +\"\\n\")\n",
    "       \n",
    "##############################################################################################\n",
    "# Loop through tables and remove duplicates of rows (bevause files are created through appending)\n",
    "\n",
    "pred_tables = glob.glob(save_pred_table_test_path + \"/*CALLTYPE_PRED_TABLE*.txt\")\n",
    "for file in pred_tables:\n",
    "    df = pd.read_csv(file, delimiter=';') \n",
    "    # df = df.drop_duplicates(keep=False)\n",
    "    df = df.loc[df['Label'] != 'Label']\n",
    "    df.to_csv(file, header=True, index=None, sep=';', mode = 'w')\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML1_env",
   "language": "python",
   "name": "ml1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
