{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERVIEW AND BACKGROUND\n",
    "\n",
    "Understanding group decision-making in mobile species is a complex challenge. The integration of both GPS and acoustic sensors into tags deployed on multiple individuals in groups can help us systematically quantify communication between individuals as well as their movement responses - based on who is calling, the context of the call, and how the group reacts. However, the resulting datasets are very large, and the manual identification and classification of calls is both time and labor consuming. \n",
    "\n",
    "This code interfaces with a supervised machine learning tool for detecting call presence and/or call type in field recordings. We train a convolutional neural network to extract features from spectrograms. Bidirectional gated recurrent units learned temporal structure from these features, and a final feed-forward network predicted call type. We also used data augmentation to increase the number of training examples for underrepresented call types. We illustrate the method on bio-logging data from meerkat groups, where all individuals in the group were equipped with acoustic loggers. \n",
    "\n",
    "# START CODING\n",
    "\n",
    "The parameter {is_forked} below is used to determine whether or not the model produces only a calltype classification (when False) or a calltype and a call presence classification (when True).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_forked = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "github_dir = \"/home/kiran/Documents/github/CCAS_ML\"\n",
    "\n",
    "# add path to local functions\n",
    "import os\n",
    "os.chdir(github_dir)\n",
    "\n",
    "# import all the params for this model\n",
    "from example_params import *\n",
    "\n",
    "# import own functions\n",
    "import preprocess.preprocess_functions as pre\n",
    "import postprocess.evaluation_metrics_functions_2 as metrics\n",
    "import postprocess.merge_predictions_functions as ppm\n",
    "import model.specgen_batch_generator as bg\n",
    "import model.network_class as rnn\n",
    "# import postprocess.visualise_prediction_functions as pp\n",
    "from model.callback_functions import LossHistory\n",
    "\n",
    "# import normal packages used in pre-processing\n",
    "import numpy as np\n",
    "import librosa\n",
    "import warnings\n",
    "import ntpath\n",
    "import os\n",
    "from itertools import compress  \n",
    "from random import random, shuffle\n",
    "from math import floor\n",
    "import statistics\n",
    "import glob\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML section packages\n",
    "import datetime\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, SeparableConv2D, concatenate\n",
    "from keras.layers import Reshape, Permute\n",
    "from keras.layers import TimeDistributed, Dense, Dropout, BatchNormalization\n",
    "from keras.models import load_model\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling2D\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# postprocessfrom decimal import Decimal\n",
    "from decimal import Decimal\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# evaluate and plot \n",
    "import seaborn as sn\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import csv\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------\n",
    "#             1 - PREPROCESSING - SETTING UP DIRECTORIES\n",
    "#----------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "# Compile all the synched label files together\n",
    "labels_all = pd.DataFrame()\n",
    "for directory in label_dirs:\n",
    "    for group in group_IDs:\n",
    "        temp = pd.read_csv(os.path.join(directory, group +\"_ALL_CALLS_SYNCHED.csv\"), sep=sep,\n",
    "                       header=0, engine = engine, encoding = encoding) \n",
    "        temp[\"group\"] = group\n",
    "        labels_all = pd.concat([labels_all, temp]) \n",
    "        del temp\n",
    "labels_all = labels_all.reset_index(drop = True)\n",
    "labels_all = labels_all[~labels_all.wavFileName.str.contains('SOUNDFOC')]\n",
    "\n",
    "#Summary data\n",
    "# labels_all.groupby(['ind','group']).size().reset_index().rename(columns={0:'count'})\n",
    "# labels_all.groupby(['ind','group', 'date']).size().reset_index().rename(columns={0:'count'})\n",
    "# labels_all.groupby(['ind', 'date']).size().reset_index().rename(columns={0:'count'})\n",
    "\n",
    "\n",
    "# subset all the audio files that we should use in the analysis (i.e. not focal follow data)\n",
    "audio_files = list(set(labels_all[\"wavFileName\"]))\n",
    "audio_filenames = list(compress(audio_files, [\"SOUNDFOC\" not in filei for filei in audio_files]))\n",
    "\n",
    "# subset all the audio files that we should use in the analysis (i.e. not focal follow data)\n",
    "label_files = list(set(labels_all[\"csvFileName\"]))\n",
    "label_filenames = list(compress(label_files, [\"SOUNDFOC\" not in filei for filei in label_files]))\n",
    "\n",
    "# get the file IDS without all the extentions (used later for naming)\n",
    "all_filenames = [audio_filenames[i].split(\".\")[0] for i in range(0,len(audio_filenames))]\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# find all the paths to the files\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "# find all the labels\n",
    "EXT = \"*.csv\"\n",
    "label_filepaths = []\n",
    "for PATH in acoustic_data_path :\n",
    "      label_filepaths.extend( [file for path, subdir, files in os.walk(PATH) for file in glob.glob(os.path.join(path, EXT))])\n",
    "EXT = \"*.CSV\"\n",
    "for PATH in acoustic_data_path :\n",
    "      label_filepaths.extend( [file for path, subdir, files in os.walk(PATH) for file in glob.glob(os.path.join(path, EXT))])\n",
    "\n",
    "# find all audio paths (will be longer than label path as not everything is labelled)\n",
    "audio_filepaths = []\n",
    "EXT = \"*.wav\"\n",
    "for PATH in audio_dirs:\n",
    "      audio_filepaths.extend( [file for path, subdir, files in os.walk(PATH) for file in glob.glob(os.path.join(path, EXT))])\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#       2 - Create a massive label table\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create the label table\n",
    "label_table = pre.create_meerkat_table(labels_all, call_types, sep,\n",
    "                                       start_column, duration_column, columns_to_keep,\n",
    "                                       label_column, convert_to_seconds, \n",
    "                                       label_for_other, label_for_noise, engine,\n",
    "                                       multiclass_forbidden)\n",
    "\n",
    "# estimate the average beep length because many of them are not annotated in the data\n",
    "avg_beep = round(statistics.mean(label_table.loc[label_table[\"beep\"],\"Duration\"].loc[label_table.loc[label_table[\"beep\"],\"Duration\"]>0]),3)\n",
    "label_table.loc[(label_table[\"beep\"].bool and label_table[\"Duration\"] == 0.) ==True, \"Duration\"] = avg_beep\n",
    "label_table.loc[(label_table[\"beep\"].bool and label_table[\"Duration\"] == avg_beep) ==True, \"End\"] += avg_beep\n",
    "\n",
    "# add wav and audio paths\n",
    "label_table[\"wav_path\"] = label_table['wavFileName'].apply(lambda x: [pathi for pathi in audio_filepaths if x in pathi][0])\n",
    "label_table[\"label_path\"] = label_table['csvFileName'].apply(lambda x: [pathi for pathi in label_filepaths if x in pathi][0])\n",
    "\n",
    "# make sure these paths are added to the noise table too\n",
    "columns_to_keep.append(\"wav_path\")\n",
    "columns_to_keep.append(\"label_path\")\n",
    "\n",
    "# create the matching noise table\n",
    "noise_table = pre.create_noise_table(label_table, call_types, label_for_noise, label_for_startstop, columns_to_keep)#, '\\$'])\n",
    "\n",
    "# remove rows where the annotated noise is smaller than the window size otherwise the spectrogram we generate will inclue a call\n",
    "noise_table = noise_table.drop(noise_table[noise_table[\"Duration\"] < spec_window_size].index)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#         1.1. Split the label filenames into training and test files\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# If the training and testing files exists then load them, otherwise create them\n",
    "if os.path.exists(os.path.join(save_model_path, \"training_files_used.txt\")):\n",
    "    # load the saved file\n",
    "    with open(os.path.join(save_model_path, \"training_files_used.txt\")) as f:\n",
    "        content = f.readlines()    \n",
    "    training_filenames = [x.strip() for x in content] # remove whitespace characters like `\\n` at the end of each line\n",
    "    with open(os.path.join(save_model_path, \"testing_files_used.txt\")) as f:\n",
    "        content = f.readlines()    \n",
    "    testing_filenames = [x.strip() for x in content] # remove whitespace characters like `\\n` at the end of each line\n",
    "    with open(os.path.join(save_model_path, \"validation_files_used.txt\")) as f:\n",
    "        content = f.readlines()    \n",
    "    validation_filenames = [x.strip() for x in content] # remove whitespace characters like `\\n` at the end of each line\n",
    "\n",
    "# otherwiss create the training and testing files\n",
    "else: \n",
    "    # randomise the order of the files\n",
    "    file_list = audio_filenames #all_filenames\n",
    "    shuffle(file_list)\n",
    "    \n",
    "    # randomly divide the files into those in the training and test datasets\n",
    "    split_index = floor(len(file_list) * train_test_split)\n",
    "    training_files = file_list[:split_index]\n",
    "    testing_filenames = file_list[split_index:]\n",
    "    \n",
    "    split_index = floor(len(training_files) * train_val_split )\n",
    "    training_filenames = training_files[:split_index]\n",
    "    validation_filenames = training_files[split_index:]\n",
    "\n",
    "    # save a copy of the training and testing diles\n",
    "    with open(os.path.join(save_model_path, \"training_files_used.txt\"), \"w\") as f:\n",
    "        for s in training_filenames:\n",
    "            f.write(str(s) +\"\\n\")\n",
    "    with open(os.path.join(save_model_path, \"testing_files_used.txt\"), \"w\") as f:\n",
    "        for s in testing_filenames:\n",
    "            f.write(str(s) +\"\\n\")\n",
    "    with open(os.path.join(save_model_path, \"validation_files_used.txt\"), \"w\") as f:\n",
    "        for s in validation_filenames:\n",
    "            f.write(str(s) +\"\\n\")\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# 2.1. Generate and save the training files\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# separate out the training and test sets for analysis\n",
    "training_label_table = label_table[label_table['wavFileName'].isin(training_filenames)]\n",
    "testing_label_table = label_table[label_table['wavFileName'].isin(testing_filenames)]\n",
    "validation_label_table = label_table[label_table['wavFileName'].isin(validation_filenames)]\n",
    "\n",
    "training_noise_table = noise_table[noise_table['wavFileName'].isin(training_filenames)]\n",
    "testing_noise_table = noise_table[noise_table['wavFileName'].isin(testing_filenames)]\n",
    "validation_noise_table = noise_table[noise_table['wavFileName'].isin(validation_filenames)]\n",
    "\n",
    "# Compile data into a format that the data generator can use\n",
    "training_label_dict = dict()\n",
    "for label in call_types: \n",
    "    training_label_dict[label] = training_label_table.loc[training_label_table[label] == True, [\"Label\", \"Start\", \"Duration\",\"End\",\"wav_path\",\"label_path\"]]\n",
    "training_label_dict[label_for_noise] = training_noise_table[[\"Label\", \"Start\", \"Duration\",\"End\",\"wav_path\",\"label_path\"]]\n",
    "\n",
    "# Compile data into a format that the data generator can use\n",
    "testing_label_dict = dict()\n",
    "for label in call_types: \n",
    "    testing_label_dict[label] = testing_label_table.loc[testing_label_table[label] == True, [\"Label\", \"Start\", \"Duration\",\"End\",\"wav_path\",\"label_path\"]]\n",
    "testing_label_dict[label_for_noise] = testing_noise_table[[\"Label\", \"Start\", \"Duration\",\"End\",\"wav_path\",\"label_path\"]]\n",
    "\n",
    "# Compile data into a format that the data generator can use\n",
    "validation_label_dict = dict()\n",
    "for label in call_types: \n",
    "    validation_label_dict[label] = validation_label_table.loc[validation_label_table[label] == True, [\"Label\", \"Start\", \"Duration\",\"End\",\"wav_path\",\"label_path\"]]\n",
    "validation_label_dict[label_for_noise] = validation_label_table[[\"Label\", \"Start\", \"Duration\",\"End\",\"wav_path\",\"label_path\"]]\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#       2.5. BATCH GENERATOR FOR TRAINING RNN\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "if is_forked == True:\n",
    "    # initiate the data generator\n",
    "    train_generator = bg.ForkedDataGenerator(training_label_dict,\n",
    "                                             training_label_table, \n",
    "                                             spec_window_size,\n",
    "                                             n_mels, \n",
    "                                             window, \n",
    "                                             fft_win , \n",
    "                                             fft_hop , \n",
    "                                             normalise,\n",
    "                                             label_for_noise,\n",
    "                                             label_for_other,\n",
    "                                             min_scaling_factor,\n",
    "                                             max_scaling_factor,\n",
    "                                             n_per_call,\n",
    "                                             other_ignored_in_training,\n",
    "                                             mask_value,\n",
    "                                             mask_vector)\n",
    "    \n",
    "        \n",
    "    # generate an example spectrogram and label  \n",
    "    spec, label, callmat, mask = train_generator.generate_example(\"sn\", 0, True)\n",
    "else:\n",
    "    # initiate the data generator\n",
    "    train_generator = bg.DataGenerator(training_label_dict,\n",
    "                                             training_label_table, \n",
    "                                             spec_window_size,\n",
    "                                             n_mels, \n",
    "                                             window, \n",
    "                                             fft_win , \n",
    "                                             fft_hop , \n",
    "                                             normalise,\n",
    "                                             label_for_noise,\n",
    "                                             label_for_other,\n",
    "                                             min_scaling_factor,\n",
    "                                             max_scaling_factor,\n",
    "                                             n_per_call,\n",
    "                                             other_ignored_in_training,\n",
    "                                             mask_value,\n",
    "                                             mask_vector)\n",
    "    \n",
    "        \n",
    "    # generate an example spectrogram and label  \n",
    "    spec, label, mask = train_generator.generate_example(\"sn\", 0, True)\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# Do some plotting to ensure it makes sense\n",
    "\n",
    "# set the labels\n",
    "label_list = list(call_types.keys())\n",
    "if other_ignored_in_training:\n",
    "    label_list.remove(label_for_other)\n",
    "\n",
    "if is_forked == True:\n",
    "    # plot spectrogram\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(411)\n",
    "    yaxis = range(0, np.flipud(spec).shape[0]+1)\n",
    "    xaxis = range(0, np.flipud(spec).shape[1]+1)\n",
    "    librosa.display.specshow(spec,  y_axis='mel', x_coords = label.columns)#, x_axis= \"time\",sr=sr, x_coords = label.columns)\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.clim(-35, 35)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    \n",
    "    # plot LABEL\n",
    "    plt.subplot(412)\n",
    "    xaxis = range(0, np.flipud(label).shape[1]+1)\n",
    "    yaxis = range(0, np.flipud(label).shape[0]+1)\n",
    "    plt.yticks(np.arange(0.5, len(label_list)+0.5 ,1 ),reversed(label_list))\n",
    "    plt.xticks(np.arange(0, np.flipud(label).shape[1]+1,50),\n",
    "               list(label.columns[np.arange(0, np.flipud(label).shape[1]+1,50)]))\n",
    "    plt.pcolormesh(xaxis, yaxis, np.flipud(label))\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Calltype')\n",
    "    plt.colorbar(label=\"Label\")\n",
    "    \n",
    "    \n",
    "    # plot call matrix\n",
    "    plt.subplot(413)\n",
    "    xaxis = range(0, np.flipud(label).shape[1]+1)\n",
    "    yaxis = range(0, np.flipud(callmat).shape[0]+1)\n",
    "    plt.yticks(np.arange(0.5, callmat.shape[0]+0.5 ,1 ), reversed(callmat.index.values))\n",
    "    plt.xticks(np.arange(0, np.flipud(label).shape[1]+1,50),\n",
    "               list(label.columns[np.arange(0, np.flipud(label).shape[1]+1,50)]))\n",
    "    plt.pcolormesh(xaxis, yaxis, np.flipud(callmat))\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Call / No Call')\n",
    "    plt.colorbar(label=\"Label\")\n",
    "    # \n",
    "    \n",
    "    plt.subplot(414)\n",
    "    # plot the mask\n",
    "    if mask_vector == True:\n",
    "        test = np.asarray([int(x == True) for x in mask])\n",
    "        # test = np.hstack(test)\n",
    "        test = test[np.newaxis, ...]\n",
    "        # plt.imshow(mask, aspect='auto', cmap=plt.cm.gray)\n",
    "        xaxis = range(0, np.flipud(label).shape[1]+1)\n",
    "        yaxis = range(0, np.flipud(test).shape[0]+1)\n",
    "        # yaxis = range(0, 1)\n",
    "        # plt.yticks(np.arange(0.5, 1 ,1 ), \"oth\")\n",
    "        plt.yticks(np.arange(0.5, test.shape[0]+0.5 ,1 ), reversed([\"oth\"]))\n",
    "        plt.xticks(np.arange(0, np.flipud(label).shape[1]+1,50),\n",
    "                   list(label.columns[np.arange(0, np.flipud(label).shape[1]+1,50)]))\n",
    "        plt.pcolormesh(xaxis, yaxis, np.flipud(test))\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('True/False')\n",
    "        plt.colorbar(label=\"Label\")\n",
    "        plt.clim(0, 1)\n",
    "        plt.show()\n",
    "    else:\n",
    "    \n",
    "        xaxis = range(0, np.flipud(mask).shape[1]+1)\n",
    "        yaxis = range(0, np.flipud(mask).shape[0]+1)\n",
    "        # plt.yticks(np.arange(0.5, np.flipud(mask).shape[0]+0.5 ,1 ),reversed(label_list))\n",
    "        plt.xticks(np.arange(0, np.flipud(mask).shape[1]+1,50),\n",
    "                   list(label.columns[np.arange(0, np.flipud(mask).shape[1]+1,50)]))\n",
    "        plt.pcolormesh(xaxis, yaxis, np.flipud(mask))\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Spectrogram Channels')\n",
    "        plt.colorbar(label=\"Other or not\")\n",
    "        plt.show()\n",
    "else:\n",
    "    # plot spectrogram\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(311)\n",
    "    yaxis = range(0, np.flipud(spec).shape[0]+1)\n",
    "    xaxis = range(0, np.flipud(spec).shape[1]+1)\n",
    "    librosa.display.specshow(spec,  y_axis='mel', x_coords = label.columns)#, x_axis= \"time\",sr=sr, x_coords = label.columns)\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.clim(-35, 35)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    \n",
    "    # plot LABEL\n",
    "    plt.subplot(312)\n",
    "    xaxis = range(0, np.flipud(label).shape[1]+1)\n",
    "    yaxis = range(0, np.flipud(label).shape[0]+1)\n",
    "    plt.yticks(np.arange(0.5, len(label_list)+0.5 ,1 ),reversed(label_list))\n",
    "    plt.xticks(np.arange(0, np.flipud(label).shape[1]+1,50),\n",
    "               list(label.columns[np.arange(0, np.flipud(label).shape[1]+1,50)]))\n",
    "    plt.pcolormesh(xaxis, yaxis, np.flipud(label))\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Calltype')\n",
    "    plt.colorbar(label=\"Label\")\n",
    "    \n",
    "    \n",
    "    plt.subplot(313)\n",
    "    # plot the mask\n",
    "    if mask_vector == True:\n",
    "        test = np.asarray([int(x == True) for x in mask])\n",
    "        # test = np.hstack(test)\n",
    "        test = test[np.newaxis, ...]\n",
    "        # plt.imshow(mask, aspect='auto', cmap=plt.cm.gray)\n",
    "        xaxis = range(0, np.flipud(label).shape[1]+1)\n",
    "        yaxis = range(0, np.flipud(test).shape[0]+1)\n",
    "        # yaxis = range(0, 1)\n",
    "        # plt.yticks(np.arange(0.5, 1 ,1 ), \"oth\")\n",
    "        plt.yticks(np.arange(0.5, test.shape[0]+0.5 ,1 ), reversed([\"oth\"]))\n",
    "        plt.xticks(np.arange(0, np.flipud(label).shape[1]+1,50),\n",
    "                   list(label.columns[np.arange(0, np.flipud(label).shape[1]+1,50)]))\n",
    "        plt.pcolormesh(xaxis, yaxis, np.flipud(test))\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('True/False')\n",
    "        plt.colorbar(label=\"Label\")\n",
    "        plt.clim(0, 1)\n",
    "        plt.show()\n",
    "    else:\n",
    "        xaxis = range(0, np.flipud(mask).shape[1]+1)\n",
    "        yaxis = range(0, np.flipud(mask).shape[0]+1)\n",
    "        # plt.yticks(np.arange(0.5, np.flipud(mask).shape[0]+0.5 ,1 ),reversed(label_list))\n",
    "        plt.xticks(np.arange(0, np.flipud(mask).shape[1]+1,50),\n",
    "                   list(label.columns[np.arange(0, np.flipud(mask).shape[1]+1,50)]))\n",
    "        plt.pcolormesh(xaxis, yaxis, np.flipud(mask))\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Spectrogram Channels')\n",
    "        plt.colorbar(label=\"Other or not\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#       3. CONSTRUCT THE RNN\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "if is_forked == True:\n",
    "    # get a batch to estimate rnn parameters\n",
    "    x_train, y_train = train_generator.__next__()#__getitem__(0)\n",
    "    \n",
    "    # initial parameters\n",
    "    num_calltypes = y_train[0].shape[2]\n",
    "    gru_units = y_train[0].shape[1] \n",
    "\n",
    "\n",
    "    # initialise the training data generator and validation data generator\n",
    "    train_generator = bg.ForkedDataGenerator(training_label_dict,\n",
    "                                             training_label_table, \n",
    "                                             spec_window_size,\n",
    "                                             n_mels, \n",
    "                                             window, \n",
    "                                             fft_win , \n",
    "                                             fft_hop , \n",
    "                                             normalise,\n",
    "                                             label_for_noise,\n",
    "                                             label_for_other,\n",
    "                                             min_scaling_factor,\n",
    "                                             max_scaling_factor,\n",
    "                                             n_per_call,\n",
    "                                             other_ignored_in_training,\n",
    "                                             mask_value,\n",
    "                                             mask_vector)\n",
    "    \n",
    "    val_generator = bg.ForkedDataGenerator(validation_label_dict,\n",
    "                                           validation_label_table, \n",
    "                                           spec_window_size,\n",
    "                                           n_mels, \n",
    "                                           window, \n",
    "                                           fft_win , \n",
    "                                           fft_hop , \n",
    "                                           normalise,\n",
    "                                           label_for_noise,\n",
    "                                           label_for_other,\n",
    "                                           min_scaling_factor,\n",
    "                                           max_scaling_factor,\n",
    "                                           n_per_call,\n",
    "                                           other_ignored_in_training,\n",
    "                                           mask_value,\n",
    "                                           mask_vector)\n",
    "    \n",
    "else:\n",
    "    x_train, y_train = train_generator.__next__()#__getitem__(0)\n",
    "    \n",
    "    # initial parameters\n",
    "    num_calltypes = y_train[0].shape[2]\n",
    "    gru_units = y_train[0].shape[1] \n",
    "    \n",
    "    train_generator = bg.DataGenerator(training_label_dict,\n",
    "                                             training_label_table, \n",
    "                                             spec_window_size,\n",
    "                                             n_mels, \n",
    "                                             window, \n",
    "                                             fft_win , \n",
    "                                             fft_hop , \n",
    "                                             normalise,\n",
    "                                             label_for_noise,\n",
    "                                             label_for_other,\n",
    "                                             min_scaling_factor,\n",
    "                                             max_scaling_factor,\n",
    "                                             n_per_call,\n",
    "                                             other_ignored_in_training,\n",
    "                                             mask_value,\n",
    "                                             mask_vector)\n",
    "    \n",
    "    val_generator = bg.DataGenerator(validation_label_dict,\n",
    "                                           validation_label_table, \n",
    "                                           spec_window_size,\n",
    "                                           n_mels, \n",
    "                                           window, \n",
    "                                           fft_win , \n",
    "                                           fft_hop , \n",
    "                                           normalise,\n",
    "                                           label_for_noise,\n",
    "                                           label_for_other,\n",
    "                                           min_scaling_factor,\n",
    "                                           max_scaling_factor,\n",
    "                                           n_per_call,\n",
    "                                           other_ignored_in_training,\n",
    "                                           mask_value,\n",
    "                                           mask_vector)    \n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "# Construct the RNN\n",
    "#--------------------------------------------\n",
    "\n",
    "# initialise the model class\n",
    "model = rnn.BuildNetwork(x_train, num_calltypes, filters, gru_units, dense_neurons, dropout, mask_value)\n",
    "\n",
    "# build the model\n",
    "if is_forked == True:\n",
    "    RNN_model = model.build_forked_masked_rnn()\n",
    "else:\n",
    "    RNN_model = model.build_masked_rnn()\n",
    "    \n",
    "# Adam optimiser\n",
    "adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile the model\n",
    "RNN_model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "# Setup callbycks: learning rate / loss /tensorboard\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True, verbose=1)\n",
    "reduce_lr_plat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=25, verbose=1,\n",
    "                                   mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.000001)\n",
    "loss = LossHistory()\n",
    "\n",
    "# tensorboard\n",
    "tensorboard = TensorBoard(log_dir = save_tensorboard_path,\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,  # Show the network\n",
    "                          write_grads=True   # Show gradients\n",
    "                          )    \n",
    "\n",
    "\n",
    "# fit model\n",
    "RNN_model.fit_generator(train_generator, \n",
    "                        steps_per_epoch = train_generator.__len__(),\n",
    "                        epochs = epochs,\n",
    "                        callbacks = [early_stopping, reduce_lr_plat, loss, tensorboard],\n",
    "                        validation_data = val_generator,\n",
    "                        validation_steps = val_generator.__len__())\n",
    "\n",
    " \n",
    "\n",
    "# save the model\n",
    "date_time = datetime.datetime.now()\n",
    "date_now = str(date_time.date())\n",
    "time_now = str(date_time.time())\n",
    "sf = os.path.join(save_model_path, run_name+ \"_\" + date_now + \"_\" + time_now)\n",
    "if not os.path.isdir(sf):\n",
    "        os.makedirs(sf)\n",
    "\n",
    "RNN_model.save(sf + '/savedmodel' + '.h5')\n",
    "\n",
    "# #----------------------------------------------------------------------------------\n",
    "# #----------------------------------------------------------------------------------\n",
    "# #               LOOP AND PREDICT OVER TEST FILES\n",
    "# #----------------------------------------------------------------------------------\n",
    "# #----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "skipped_files = []\n",
    "\n",
    "for file_ID in testing_filenames:\n",
    "    # file_ID = testing_filenames[0]\n",
    "    \n",
    "    # subset the label table to only use that one file\n",
    "    label_table = testing_label_table[testing_label_table['wavFileName'].isin([file_ID])].reset_index()\n",
    "    # Find the file ID name i.e. remove the extention\n",
    "    file_ID = file_ID.split(\".\")[0] \n",
    "    # find the matching audio for the label data\n",
    "    audio_path = label_table[\"wav_path\"][0]   \n",
    "    \n",
    "    print(\"*****************************************************************\")   \n",
    "    print(\"*****************************************************************\") \n",
    "    print (\"File being processed : \" + audio_path)    \n",
    "    \n",
    "    # find the start and stop  of the labelling periods (also using skipon/skipoff)\n",
    "    loop_table = label_table.loc[label_table[\"Label\"].str.contains('|'.join(label_for_startstop), regex=True, case = False), [\"Label\",\"Start\"]]\n",
    "    loop_times = list(loop_table[\"Start\"])\n",
    "    \n",
    "    # Make sure that the file contains the right number of start and stops, otherwise go to the next file\n",
    "    if len(loop_times)%2 != 0:\n",
    "        print(\"!!!!!!!!!!!!!!!!\")\n",
    "        warnings.warn(\"There is a missing start or stop in this file and it has been skipped: \" + audio_path)\n",
    "        skipped_files.append(file_ID)\n",
    "        continue \n",
    "    \n",
    "    if len(loop_times) == 0:\n",
    "        print(\"!!!!!!!!!!!!!!!!\")\n",
    "        warnings.warn(\"There is a missing start or stop in this file and it has been skipped: \" + audio_path)\n",
    "        skipped_files.append(file_ID)\n",
    "        continue \n",
    "    \n",
    "    # save the label_table\n",
    "    save_label_table_filename = file_ID + \"_LABEL_TABLE.txt\"\n",
    "    label_table.to_csv(os.path.join(save_label_table_test_path, save_label_table_filename), \n",
    "                        header=True, index=None, sep=';')\n",
    "    \n",
    "    # load the audio data\n",
    "    y, sr = librosa.load(audio_path, sr=None, mono=False)\n",
    "    \n",
    "    # # Reshaping the Audio file (mono) to deal with all wav files similarly\n",
    "    # if y.ndim == 1:\n",
    "    #     y = y.reshape(1, -1)\n",
    "    \n",
    "    # # Implement this for acc data\n",
    "    # for ch in range(y.shape[0]):\n",
    "    # ch=0\n",
    "    # y_sub = y[:,ch]\n",
    "    y_sub = y\n",
    "    \n",
    "    # loop through every labelling start based on skipon/off within this loop_table\n",
    "    for loopi in range(0, int(len(loop_times)), 2):\n",
    "        # loopi = 0\n",
    "        fromi =  loop_times[loopi]\n",
    "        #toi = fromi + 5\n",
    "        toi = loop_times[int(loopi + 1)] # define the end of the labelling periods\n",
    "        \n",
    "        # if the file exists, load it\n",
    "        if os.path.exists(os.path.join(save_pred_stack_test_path, file_ID + '_CALLTYPE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy')): \n",
    "            calltype_pred_list = np.load( os.path.join(save_pred_stack_test_path, file_ID + '_CALLTYPE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy')) \n",
    "            callpresence_pred_list = np.load( os.path.join(save_pred_stack_test_path, file_ID + '_CALLPRESENCE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy'))\n",
    "       \n",
    "        # if not, generate it\n",
    "        else:\n",
    "        \n",
    "            calltype_pred_list = []\n",
    "            callpresence_pred_list = []\n",
    "    \n",
    "            for spectro_slide in np.arange(fromi, toi, slide):\n",
    "                \n",
    "                # spectro_slide = fromi\n",
    "                start = round(spectro_slide,3)\n",
    "                stop = round(spectro_slide + spec_window_size, 3)\n",
    "                \n",
    "                # ignore cases where the window is larger than what is labelled (e.g. at the end)\n",
    "                if stop <= toi:\n",
    "                    \n",
    "                    #generate the spectrogram\n",
    "                    spectro = pre.generate_mel_spectrogram(y=y_sub, sr=sr, start=start, stop=stop, \n",
    "                                                            n_mels = n_mels, window='hann', \n",
    "                                                            fft_win= fft_win, fft_hop = fft_hop, \n",
    "                                                            normalise = True)\n",
    "                    \n",
    "                    # transpose it and put it in a format that works with the NN\n",
    "                    spec = spectro.T\n",
    "                    spec = spec[np.newaxis, ..., np.newaxis]  \n",
    "                    \n",
    "                    # generate a mask (as a placeholder) but don't mask anything as we are predicting and want to include other\n",
    "                    mask = np.asarray([True for i in range(spectro.shape[1])])\n",
    "                    mask = mask[np.newaxis,...]\n",
    "                    \n",
    "                    # generate the prediction\n",
    "                    pred = RNN_model.predict([spec,mask])\n",
    "                                       \n",
    "                    # add this prediction to the stack that will be used to generate the predictions table\n",
    "                    calltype_pred_list.append(np.squeeze(pred[0]))\n",
    "                    callpresence_pred_list.append(np.squeeze(pred[1]))\n",
    "                    \n",
    "            # save the prediction stacks\n",
    "            np.save( os.path.join(save_pred_stack_test_path, file_ID + '_CALLTYPE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy'), calltype_pred_list)\n",
    "            with open(os.path.join(save_pred_stack_test_path, file_ID + '_CALLTYPE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.txt'), \"w\") as f:\n",
    "                for row in calltype_pred_list:\n",
    "                    f.write(str(row) +\"\\n\")\n",
    "                    \n",
    "            np.save( os.path.join(save_pred_stack_test_path, file_ID + '_CALLPRESENCE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy'), callpresence_pred_list)\n",
    "            with open(os.path.join(save_pred_stack_test_path, file_ID + '_CALLPRESENCE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.txt'), \"w\") as f:\n",
    "                for row in callpresence_pred_list:\n",
    "                    f.write(str(row) +\"\\n\")\n",
    "                \n",
    "        # Loop through different sets of thresholds\n",
    "        for low_thr in [0.1,0.2, 0.3]:\n",
    "            for high_thr in [0.3,0.4,0.5,0.6,0.5,0.7,0.8,0.9,0.95]: \n",
    "                \n",
    "                # make sure it doesnt generate a 0.00098982374957839486 type number\n",
    "                low_thr = round(low_thr,2)                               \n",
    "                high_thr = round(high_thr,2)\n",
    "                \n",
    "                # stop the loop if the low threshold is bigger than the high threshold\n",
    "                if low_thr >= high_thr:\n",
    "                    continue\n",
    "\n",
    "                save_pred_table_filename = file_ID + \"_CALLTYPE_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + \".txt\"\n",
    "                \n",
    "                # if the file exists, pass to the next iteration of the loop\n",
    "                if os.path.exists(os.path.join(save_pred_table_test_path, save_pred_table_filename)):\n",
    "                    continue\n",
    "                \n",
    "                print(\"*****************************************************************\") \n",
    "                print (\"Low Threshold: \" + str(low_thr))    \n",
    "                print (\"High Threshold: \" + str(high_thr))  \n",
    "                \n",
    "                #----------------------------------------------------------------------------\n",
    "                # Compile the predictions for each on/off labelling chunk\n",
    "                detections = ppm.merge_p(probabilities = calltype_pred_list, \n",
    "                                          labels=list(call_types.keys()),\n",
    "                                          starttime = 0, \n",
    "                                          frameadv_s = fft_hop, \n",
    "                                          specadv_s = slide,\n",
    "                                          low_thr=low_thr, \n",
    "                                          high_thr=high_thr, \n",
    "                                          debug=1)\n",
    "                \n",
    "                #in case the dataset was just noise, still create an empty placeholder to merge\n",
    "                if len(detections) == 0:  \n",
    "                    detections = pd.DataFrame(columns = ['category', 'start', 'end', 'scores'])\n",
    "                \n",
    "                # create an empty dataset\n",
    "                pred_table = pd.DataFrame() \n",
    "                \n",
    "                #convert these detections to a predictions table                \n",
    "                table = pd.DataFrame(detections)\n",
    "                table[\"Label\"] = table[\"category\"]\n",
    "                table[\"Start\"] = round(table[\"start\"]*fft_hop + fromi, 3) #table[\"start\"].apply(Decimal)*Decimal(fft_hop) + Decimal(fromi)\n",
    "                table[\"Duration\"] = round( (table[\"end\"]-table[\"start\"])*fft_hop, 3) #(table[\"end\"].apply(Decimal)-table[\"start\"].apply(Decimal))*Decimal(fft_hop)\n",
    "                table[\"End\"] = round(table[\"end\"]*fft_hop + fromi, 3) #table[\"Start\"].apply(Decimal) + table[\"Duration\"].apply(Decimal)\n",
    "                \n",
    "                # keep only the useful columns    \n",
    "                table = table[[\"Label\",\"Start\",\"Duration\", \"End\", \"scores\"]]  \n",
    "                \n",
    "                # Add a row which stores the start of the labelling period\n",
    "                row_start = pd.DataFrame()\n",
    "                row_start.loc[0,'Label'] = list(loop_table[\"Label\"])[loopi]\n",
    "                row_start.loc[0,'Start'] = fromi\n",
    "                row_start.loc[0,'Duration'] = 0\n",
    "                row_start.loc[0,'End'] = fromi \n",
    "                row_start.loc[0,'scores'] = None\n",
    "                \n",
    "                # Add a row which stores the end of the labelling period\n",
    "                row_stop = pd.DataFrame()\n",
    "                row_stop.loc[0,'Label'] = list(loop_table[\"Label\"])[int(loopi + 1)]\n",
    "                row_stop.loc[0,'Start'] = toi\n",
    "                row_stop.loc[0,'Duration'] = 0\n",
    "                row_stop.loc[0,'End'] = toi \n",
    "                row_start.loc[0,'scores'] = None\n",
    "                \n",
    "                # put these rows to the label table\n",
    "                table = pd.concat([row_start, table, row_stop]) \n",
    "                \n",
    "                # add the true false columns based on the call types dictionary\n",
    "                for true_label in call_types:\n",
    "                    table[true_label] = False\n",
    "                    for old_label in call_types[true_label]:\n",
    "                        table.loc[table[\"Label\"].str.contains(old_label, regex=True, case = False), true_label] = True\n",
    "                \n",
    "                # add this table to the overall predictions table for that collar\n",
    "                pred_table = pd.concat([pred_table, table ])\n",
    "                \n",
    "                # for each on/off labelling chunk, we can save the prediction and append it to the previous chunk\n",
    "                if loopi == 0:                    \n",
    "                    # for the first chunck keep the header, but not when appending later\n",
    "                    pred_table.to_csv(os.path.join(save_pred_table_test_path, save_pred_table_filename), \n",
    "                                      header=True, index=None, sep=';', mode = 'a')\n",
    "                else:\n",
    "                    pred_table.to_csv(os.path.join(save_pred_table_test_path, save_pred_table_filename), \n",
    "                                      header=None, index=None, sep=';', mode = 'a')\n",
    "                \n",
    "# '''\n",
    "# # load the saved file\n",
    "# with open(os.path.join(save_pred_stack_test_path, file_ID + '_PRED_STACK.txt')) as f:\n",
    "#     content = f.readlines()\n",
    "# # remove whitespace characters like `\\n` at the end of each line\n",
    "# pred_list = [x.strip() for x in content] \n",
    "\n",
    "\n",
    "# #or\n",
    "# pred_list = np.load( os.path.join(save_pred_stack_test_path, file_ID + '_PRED_STACK.npy'))\n",
    "# '''\n",
    "        \n",
    "# save the files that were skipped\n",
    "print(skipped_files)\n",
    "\n",
    "# save a copy of the training and testing diles\n",
    "with open(os.path.join(save_model_path, \"skipped_testing_files.txt\"), \"w\") as f:\n",
    "    for s in skipped_files:\n",
    "        f.write(str(s) +\"\\n\")\n",
    "       \n",
    "##############################################################################################\n",
    "# Loop through tables and remove duplicates of rows (bevause files are created through appending)\n",
    "\n",
    "pred_tables = glob.glob(save_pred_table_test_path + \"/*CALLTYPE_PRED_TABLE*.txt\")\n",
    "for file in pred_tables:\n",
    "    df = pd.read_csv(file, delimiter=';') \n",
    "    # df = df.drop_duplicates(keep=False)\n",
    "    df = df.loc[df['Label'] != 'Label']\n",
    "    df.to_csv(file, header=True, index=None, sep=';', mode = 'w')\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "#\n",
    "#            EVALUATE\n",
    "#\n",
    "##############################################################################################\n",
    "\n",
    "#########################################################################\n",
    "##  Create overall thresholds\n",
    "#########################################################################\n",
    "\n",
    "file_ID_list = [file_ID.split(\".\")[0] for file_ID in testing_filenames if file_ID not in skipped_files]\n",
    "label_list =  [os.path.join(save_label_table_test_path,file_ID.split(\".\")[0]  + \"_LABEL_TABLE.txt\" ) for file_ID in file_ID_list]\n",
    "\n",
    "# because of new file format, need to only keep certain columns\n",
    "column_names = [\"Label\",\"Start\",\"Duration\",\"End\"]\n",
    "column_names.extend(list(testing_label_dict.keys()))\n",
    "for file in label_list :\n",
    "    df = pd.read_csv(file, delimiter=';') \n",
    "    # df = df.drop_duplicates(keep=False)\n",
    "    df = df[column_names]\n",
    "    df.to_csv(file, header=True, index=None, sep=';', mode = 'w')\n",
    "\n",
    "\n",
    "for low_thr in [0.1,0.2,0.3]:\n",
    "    for high_thr in [0.3,0.4,0.5,0.6,0.5,0.7,0.8,0.9,0.95]: \n",
    "        \n",
    "        low_thr = round(low_thr,2)                               \n",
    "        high_thr = round(high_thr,2) \n",
    "        \n",
    "        if low_thr >= high_thr:\n",
    "            continue\n",
    "        \n",
    "        pred_list = [os.path.join(save_pred_table_test_path,file_ID.split(\".\")[0]  + \"_CALLTYPE_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + \".txt\" ) for file_ID in file_ID_list ]\n",
    "        evaluation = metrics.Evaluate(label_list, pred_list, 0.5, 5) # 0.99 is 0.5\n",
    "        # Prec, Rec, cat_frag, time_frag, cf, gt_indices, pred_indices, match, offset = evaluation.main()\n",
    "        Prec, lenient_Prec, Rec, lenient_Rec, cat_frag, time_frag, cm, gt_indices, pred_indices, match, offset, call_match, pred_match, match2 = evaluation.main()\n",
    "        \n",
    "        # specify file names\n",
    "        precision_filename = \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + '_Precision.csv'\n",
    "        recall_filename = \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + '_Recall.csv'\n",
    "        cat_frag_filename = \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + '_Category_fragmentation.csv'\n",
    "        time_frag_filename = \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + '_Time_fragmentation.csv'\n",
    "        confusion_filename = \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + '_Confusion_matrix.csv'\n",
    "        gt_filename = \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + \"_Label_indices.csv\"\n",
    "        pred_filename = \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + \"_Prection_indices.csv\"\n",
    "        match_filename = \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + \"_Matching_table.txt\"\n",
    "        timediff_filename = \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + \"_Time_difference.txt\"    \n",
    "        \n",
    "        # NEW METRICS\n",
    "        lenient_Prec_filename  = \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + '_Lenient_Precision.csv'\n",
    "        lenient_Rec_filename = \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + '_Lenient_Recall.csv'\n",
    "        call_match_filename = \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + \"_Call_Match.txt\"\n",
    "        pred_match_filename = \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + \"_Prediction_Match.txt\"\n",
    "        match2_filename = \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + \"_Matching_table.txt\"\n",
    "        \n",
    "        lenient_Prec.to_csv(os.path.join(save_metrics_path, lenient_Prec_filename))        \n",
    "        lenient_Rec.to_csv(os.path.join(save_metrics_path, lenient_Rec_filename))\n",
    "        with open(os.path.join(save_metrics_path, call_match_filename), 'wb') as fp:\n",
    "            pickle.dump(call_match, fp)  \n",
    "        with open(os.path.join(save_metrics_path, pred_match_filename), 'wb') as fp:\n",
    "            pickle.dump(pred_match, fp)  \n",
    "        with open(os.path.join(save_metrics_path, match2_filename), 'wb') as fp:\n",
    "            pickle.dump(match2, fp)  \n",
    "        \n",
    "        \n",
    "        \n",
    "        # save files\n",
    "        Prec.to_csv( os.path.join(save_metrics_path, precision_filename))\n",
    "        Rec.to_csv( os.path.join(save_metrics_path, recall_filename))\n",
    "        cat_frag.to_csv( os.path.join(save_metrics_path, cat_frag_filename))\n",
    "        time_frag.to_csv(os.path.join(save_metrics_path, time_frag_filename))\n",
    "        cm.to_csv(os.path.join(save_metrics_path, confusion_filename))\n",
    "        gt_indices.to_csv(os.path.join(save_metrics_path, gt_filename ))\n",
    "        pred_indices.to_csv(os.path.join(save_metrics_path, pred_filename ))                  \n",
    "        with open(os.path.join(save_metrics_path, match_filename), \"wb\") as fp:   #Picklin\n",
    "            pickle.dump(match, fp)\n",
    "        with open(os.path.join(save_metrics_path, timediff_filename), \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(offset, fp)    \n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# plot overall confusion matrix\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "# loop over the threaholds\n",
    "for low_thr in [0.1,0.2,0.3]:\n",
    "    for high_thr in [0.3,0.4,0.5,0.6,0.5,0.7,0.8,0.9,0.95]: \n",
    "        \n",
    "        low_thr = round(low_thr,2)                               \n",
    "        high_thr = round(high_thr,2) \n",
    "        \n",
    "        if low_thr >= high_thr:\n",
    "            continue\n",
    "\n",
    "        confusion_filename = os.path.join(save_metrics_path, \"Overall_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + '_Confusion_matrix.csv')\n",
    "        with open(confusion_filename, newline='') as csvfile:\n",
    "            array = list(csv.reader(csvfile))\n",
    "    \n",
    "        df_cm = pd.DataFrame(array)#, range(6), range(6))    \n",
    "        \n",
    "        # get rid of the weird indentations and make rows and columns as names\n",
    "        new_col = df_cm.iloc[0] # grab the first row for the header\n",
    "        df_cm = df_cm[1:] # take the data less the header row\n",
    "        df_cm.columns = new_col # set the header row as the df header    \n",
    "        new_row = df_cm['']\n",
    "        df_cm = df_cm.drop('', 1)\n",
    "        df_cm.index = new_row\n",
    "        df_cm.index.name= None\n",
    "        df_cm.columns.name= None\n",
    "        \n",
    "        # # replace FP and FN with noise\n",
    "        df_cm['noise'] = df_cm['FN'] \n",
    "        df_cm.loc['noise']=df_cm.loc['FP']\n",
    "        \n",
    "        # remove FP and FN\n",
    "        df_cm = df_cm.drop(\"FN\", axis=1)\n",
    "        df_cm = df_cm.drop(\"FP\", axis=0)\n",
    "        \n",
    "        df_cm = df_cm.apply(pd.to_numeric)\n",
    "                \n",
    "        # Raw confusion matrix\n",
    "        df_cm = df_cm[list(testing_label_dict.keys())]\n",
    "        df_cm = df_cm.reindex(list(testing_label_dict.keys()))\n",
    "        \n",
    "        # plot raw\n",
    "        ax = plt.axes()\n",
    "        sn.set(font_scale=1.1) # for label size\n",
    "        sn.heatmap((df_cm+1), annot=df_cm, fmt='g',norm = LogNorm(), annot_kws={\"size\": 10}, ax= ax) # font size\n",
    "        ax.set_title(str(low_thr) + \"-\" + str(high_thr) )\n",
    "        plt.savefig(os.path.join(save_metrics_path, \"Confusion_mat_thr_\" + str(low_thr) + \"-\" + str(high_thr) + '.png'))\n",
    "        plt.show()\n",
    "\n",
    "        # Recall confusion matrix\n",
    "        df_recall = df_cm.div(df_cm.sum(axis=1), axis=0).round(2)#pd.DataFrame(df_cm.values / df_cm.sum(axis=1).values).round(2)\n",
    "        \n",
    "        # plot recall\n",
    "        ax = plt.axes()\n",
    "        sn.set(font_scale=1.1) # for label size\n",
    "        sn.heatmap((df_recall), annot=True, fmt='g', annot_kws={\"size\": 10}, ax= ax) # font size\n",
    "        ax.set_title(str(low_thr) + \"-\" + str(high_thr) )\n",
    "        plt.savefig(os.path.join(save_metrics_path, \"Confusion_mat_recall_thr_\" + str(low_thr) + \"-\" + str(high_thr) + '.png'))\n",
    "        plt.show()        \n",
    "        \n",
    "        # Proportion of calls for confusion matrix\n",
    "        call_len = list()\n",
    "        for i in testing_label_dict.keys():\n",
    "            call_len.append(testing_label_dict[i].shape[0])\n",
    "        # add noise at the end\n",
    "        call_len[-1] = df_cm.sum(axis=1)[-1]\n",
    "        \n",
    "        # plot proportion of calls\n",
    "        df_prop = df_cm.div(call_len, axis=0).round(2)#pd.DataFrame(df_cm.values / df_cm.sum(axis=1).values).round(2)\n",
    "        ax = plt.axes()\n",
    "        sn.set(font_scale=1.1) # for label size\n",
    "        sn.heatmap((df_prop), annot=True, fmt='g', annot_kws={\"size\": 10}, ax= ax) # font size\n",
    "        ax.set_title(str(low_thr) + \"-\" + str(high_thr) )\n",
    "        plt.savefig(os.path.join(save_metrics_path, \"Confusion_mat_prop_thr_\" + str(low_thr) + \"-\" + str(high_thr) + '.png'))\n",
    "        plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML1_env",
   "language": "python",
   "name": "ml1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
