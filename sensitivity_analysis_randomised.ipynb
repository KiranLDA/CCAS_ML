{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENSITIVITY ANALYSES\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kiran/Documents/github/CCAS_ML/postprocess/evaluation_metrics_functions_old.py:10: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/Documents/github/CCAS_ML/postprocess/evaluation_metrics_functions_old.py:12: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "github_dir = \"/home/kiran/Documents/github/CCAS_ML\"\n",
    "\n",
    "# add path to local functions\n",
    "import os\n",
    "os.chdir(github_dir)\n",
    "\n",
    "# import all the params for this model\n",
    "from example_params import *\n",
    "is_forked = True # is going to need to go into the params file\n",
    "\n",
    "# import own functions\n",
    "import preprocess.preprocess_functions as pre\n",
    "import postprocess.evaluation_metrics_functions_old as metrics\n",
    "import postprocess.merge_predictions_functions as ppm\n",
    "import model.specgen_batch_generator as bg\n",
    "import model.network_class as rnn\n",
    "# import postprocess.visualise_prediction_functions as pp\n",
    "from model.callback_functions import LossHistory\n",
    "import model.audiopool as audiopool\n",
    "\n",
    "# import normal packages used in pre-processing\n",
    "import numpy as np\n",
    "import librosa\n",
    "import warnings\n",
    "import ntpath\n",
    "import os\n",
    "from itertools import compress  \n",
    "from random import random, shuffle\n",
    "from math import floor\n",
    "import statistics\n",
    "import glob\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML section packages\n",
    "import datetime\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, SeparableConv2D, concatenate\n",
    "from keras.layers import Reshape, Permute\n",
    "from keras.layers import TimeDistributed, Dense, Dropout, BatchNormalization\n",
    "from keras.models import load_model\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling2D\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# postprocessfrom decimal import Decimal\n",
    "from decimal import Decimal\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# evaluate and plot \n",
    "import seaborn as sn\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "# PREPROCESSING / DATA WRANGLING\n",
    "\n",
    "Currently, all the meerkat files are found on the server. The labels in particular are divided by year (currently 2017 and 2019) and all file labels are in a single document. This bit of code just takes these and puts them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all the synched label files together\n",
    "labels_all = pd.DataFrame()\n",
    "for directory in label_dirs:\n",
    "    for group in group_IDs:\n",
    "        temp = pd.read_csv(os.path.join(directory, group +\"_ALL_CALLS_SYNCHED.csv\"), sep=sep,\n",
    "                       header=0, engine = engine, encoding = encoding) \n",
    "        temp[\"group\"] = group\n",
    "        labels_all = pd.concat([labels_all, temp]) \n",
    "        del temp\n",
    "\n",
    "labels_all = labels_all[-labels_all.wavFileName.str.contains('SOUNDFOC')]\n",
    "labels_all = labels_all.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data also contain focal follows (someone walking around behind the meerkats) and the resultion of this data is different and therefore not anlysed with the collar data (but could be done separately or put to the same resolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset all the audio files that we should use in the analysis (i.e. not focal follow data)\n",
    "audio_files = list(set(labels_all[\"wavFileName\"]))\n",
    "audio_filenames = list(compress(audio_files, [\"SOUNDFOC\" not in filei for filei in audio_files]))\n",
    "\n",
    "# subset all the audio files that we should use in the analysis (i.e. not focal follow data)\n",
    "label_files = list(set(labels_all[\"csvFileName\"]))\n",
    "label_filenames = list(compress(label_files, [\"SOUNDFOC\" not in filei for filei in label_files]))\n",
    "\n",
    "# get the file IDS without all the extentions (used later for naming)\n",
    "all_filenames = [audio_filenames[i].split(\".\")[0] for i in range(0,len(audio_filenames))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makeup for the fact that there are many new files\n",
    "#all_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import re\n",
    "\n",
    "root = \"/media/kiran/D0-P1/animal_data/meerkat/sensitivity_analysis\"\n",
    "pattern = \"*_LABEL_TABLE*\"\n",
    "\n",
    "all_old_filenames =list()#[y for x in os.walk(root) for y in glob.glob(pattern)]\n",
    "\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        #if re.match()\n",
    "        if fnmatch.fnmatch(file, pattern):\n",
    "        #glog.glob()\n",
    "            #print(file)\n",
    "            \n",
    "            all_old_filenames.append(re.sub(\"_LABEL_TABLE.txt\",\"\",file)+\".wav\") \n",
    "\n",
    "#all_old_filenames=[files for path, subdirs, files in os.walk(root) ]#[file for file in glob.glob(pattern)]#\n",
    "#all_old_filenames = [x for xs in all_old_filenames for x in xs]\n",
    "all_old_filenames = list(set(all_old_filenames ))\n",
    "#all_old_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_old_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we locate all the paths to the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the labels\n",
    "EXT = \"*.csv\"\n",
    "label_filepaths = []\n",
    "for PATH in acoustic_data_path :\n",
    "      label_filepaths.extend( [file for path, subdir, files in os.walk(PATH) for file in glob.glob(os.path.join(path, EXT))])\n",
    "EXT = \"*.CSV\"\n",
    "for PATH in acoustic_data_path :\n",
    "      label_filepaths.extend( [file for path, subdir, files in os.walk(PATH) for file in glob.glob(os.path.join(path, EXT))])\n",
    "\n",
    "# find all audio paths (will be longer than label path as not everything is labelled)\n",
    "audio_filepaths = []\n",
    "EXT = \"*.wav\"\n",
    "for PATH in audio_dirs:\n",
    "      audio_filepaths.extend( [file for path, subdir, files in os.walk(PATH) for file in glob.glob(os.path.join(path, EXT))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a label table\n",
    "\n",
    "Currently, the labels are stored in a file generated by audition for the meerkats. We want to these manual labels and put them into a more meaningful categories for for the machine learning. To set categories, I use a pre-defined dictionary called call_types that is defined in the parameters file and which specifies what the different classes are for the call types. Anything strange gets put into a category \"oth\" for other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Create the label table\n",
    "label_table = pre.create_meerkat_table(labels_all, call_types, sep,\n",
    "                                       start_column, duration_column, columns_to_keep,\n",
    "                                       label_column, convert_to_seconds, \n",
    "                                       label_for_other, label_for_noise, engine,\n",
    "                                       multiclass_forbidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07\n"
     ]
    }
   ],
   "source": [
    "# estimate the average beep length because many of them are not annotated in the data\n",
    "avg_beep = round(statistics.mean(label_table.loc[label_table[\"beep\"],\"Duration\"].loc[label_table.loc[label_table[\"beep\"],\"Duration\"]>0]),3)\n",
    "label_table.loc[(label_table[\"beep\"].bool and label_table[\"Duration\"] == 0.) ==True, \"Duration\"] = avg_beep\n",
    "label_table.loc[(label_table[\"beep\"].bool and label_table[\"Duration\"] == avg_beep) ==True, \"End\"] += avg_beep\n",
    "print(avg_beep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add wav and audio paths\n",
    "label_table[\"wav_path\"] = label_table['wavFileName'].apply(lambda x: [pathi for pathi in audio_filepaths if x in pathi][0])\n",
    "label_table[\"label_path\"] = label_table['csvFileName'].apply(lambda x: [pathi for pathi in label_filepaths if x in pathi][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure these paths are added to the noise table too\n",
    "columns_to_keep.append(\"wav_path\")\n",
    "columns_to_keep.append(\"label_path\")\n",
    "\n",
    "# create the matching noise table\n",
    "noise_table = pre.create_noise_table(label_table, call_types, label_for_noise, label_for_startstop, columns_to_keep)#, '\\$'])\n",
    "\n",
    "# remove rows where the annotated noise is smaller than the window size otherwise the spectrogram we generate will inclue a call\n",
    "noise_table = noise_table.drop(noise_table[noise_table[\"Duration\"] < spec_window_size].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENSITIVITY ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audio_filenames Here we split the training and test set based on files (rather than spectrograms). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pattern = [[\"L2019\",\"HM2019\"],[\"HM2017\",\"HM2019\"], [\"L2019\"], [\"HM2017\"]]\n",
    "#train_pattern = label_table[\"date\"].unique()\n",
    "#train_pattern = [[str(i)] for i in list(train_pattern )]\n",
    "#train_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kiran/anaconda3/envs/ML1_env/lib/python3.7/site-packages/keras/callbacks.py:995: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ed81df980323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    549\u001b[0m                                               \u001b[0mlow_thr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow_thr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m                                               \u001b[0mhigh_thr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhigh_thr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                                               debug=1)\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                     \u001b[0;31m#in case the dataset was just noise, still create an empty placeholder to merge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/github/CCAS_ML/postprocess/merge_predictions_functions.py\u001b[0m in \u001b[0;36mmerge_p\u001b[0;34m(probabilities, labels, starttime, frameadv_s, specadv_s, low_thr, high_thr, debug)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Get number of score matrices, number of categories and frames per score matrix (assume uniform)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mframesN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatsN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mspecadv_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecadv_s\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mframeadv_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "for sensitivity_i in range(10):  \n",
    "    #sensitivity_i = ['VHMM003']\n",
    "\n",
    "\n",
    "    ### SETUP file structure for sensitivity runs\n",
    "    #------------------------------------------------------------\n",
    "    run_name = \"NoiseAugmented_\"+ str(min_scaling_factor)+\"_\" +str(max_scaling_factor)+\"_NotWeighted_MaskedOther_Forked_trained_with_\" + '_random_'+str(sensitivity_i)\n",
    "\n",
    "    # basically the root directory for train, test and model\n",
    "    save_data_path = os.path.join(results_dir, run_name)\n",
    "    if not os.path.isdir(save_data_path):\n",
    "        os.makedirs(save_data_path)\n",
    "\n",
    "    # Test folders\n",
    "    test_path = os.path.join(save_data_path, 'test_data')\n",
    "    if not os.path.isdir(test_path):\n",
    "        os.makedirs(test_path)\n",
    "\n",
    "\n",
    "    save_pred_test_path = os.path.join(test_path , \"predictions\")\n",
    "    if not os.path.isdir(save_pred_test_path):\n",
    "        os.makedirs(save_pred_test_path)\n",
    "\n",
    "    save_pred_stack_test_path = os.path.join(save_pred_test_path,\"stacks\")\n",
    "    if not os.path.isdir(save_pred_stack_test_path):\n",
    "        os.makedirs(save_pred_stack_test_path)        \n",
    "\n",
    "    save_pred_table_test_path = os.path.join(save_pred_test_path,\"pred_table\")\n",
    "    if not os.path.isdir(save_pred_table_test_path):\n",
    "        os.makedirs(save_pred_table_test_path)\n",
    "\n",
    "    save_label_table_test_path = os.path.join(test_path, 'label_table')\n",
    "    if not os.path.isdir(save_label_table_test_path):\n",
    "        os.makedirs(save_label_table_test_path)\n",
    "\n",
    "    save_metrics_path = os.path.join(test_path , \"metrics\")\n",
    "    if not os.path.isdir(save_metrics_path):\n",
    "        os.makedirs(save_metrics_path)\n",
    "\n",
    "    save_metrics_path_eval = os.path.join(save_metrics_path, eval_analysis)\n",
    "    if not os.path.isdir(save_metrics_path_eval):\n",
    "        os.makedirs(save_metrics_path_eval)\n",
    "\n",
    "\n",
    "    # Model folder\n",
    "    save_model_path = os.path.join(save_data_path, 'trained_model')\n",
    "    if not os.path.isdir(save_model_path):\n",
    "        os.makedirs(save_model_path)\n",
    "    \n",
    "    ### randomise files used in training and testing\n",
    "    file_list = all_old_filenames\n",
    "    shuffle(file_list)\n",
    "    \n",
    "    # randomly divide the files into those in the training and test datasets\n",
    "    split_index = floor(len(file_list) * train_test_split)\n",
    "    training_files = file_list[:split_index]\n",
    "    testing_filenames = file_list[split_index:]\n",
    "    \n",
    "    '''\n",
    "    split_index = floor(len(training_files) * train_val_split )\n",
    "    training_filenames = training_files[:split_index]\n",
    "    validation_filenames = training_files[split_index:]\n",
    "    '''\n",
    "\n",
    "    #training_files = label_table['wavFileName'][label_table['date'].isin(sensitivity_i)].unique()\n",
    "    #testing_filenames = label_table['wavFileName'][label_table['date'].isin(sensitivity_i) == False].unique()\n",
    "\n",
    "\n",
    "\n",
    "    if len(training_files) > 100:\n",
    "        shuffle(training_files)\n",
    "        training_files = training_files[0:100]\n",
    "\n",
    "    if len(testing_filenames) > 11:\n",
    "        shuffle(testing_filenames)\n",
    "        #validation_filenames = testing_filenames[:4]\n",
    "        testing_filenames = testing_filenames[0:11]\n",
    "\n",
    "\n",
    "    # Split the training and test set based on the separation\n",
    "    split_index = floor(len(training_files) * train_val_split )\n",
    "    shuffle(training_files)\n",
    "    training_filenames = training_files#[:split_index] # am no longer separating the training and val here because\n",
    "    #validation_filenames = training_files[split_index:]\n",
    "\n",
    "\n",
    "    # save a copy of the training and testing diles\n",
    "    with open(os.path.join(save_model_path, \"training_files_used.txt\"), \"w\") as f:\n",
    "        for s in training_filenames:\n",
    "            f.write(str(s) +\"\\n\")\n",
    "    with open(os.path.join(save_model_path, \"testing_files_used.txt\"), \"w\") as f:\n",
    "        for s in testing_filenames:\n",
    "            f.write(str(s) +\"\\n\")\n",
    "    # with open(os.path.join(save_model_path, \"validation_files_used.txt\"), \"w\") as f:\n",
    "    #    for s in validation_filenames:\n",
    "    #       f.write(str(s) +\"\\n\")\n",
    "\n",
    "\n",
    "    '''Then we create a dictionary that will be used in the datagenerator. \n",
    "    Each key is a calltype and contains the start/stop/duration/filename where that call occurs. \n",
    "    This allows the data generator to shuffle them during the training.\n",
    "    '''\n",
    "\n",
    "    # separate out the training and test sets for analysis\n",
    "    training_label_full_table = label_table[label_table['wavFileName'].isin(training_filenames)]\n",
    "    testing_label_table = label_table[label_table['wavFileName'].isin(testing_filenames)]\n",
    "    #validation_label_table = label_table[label_table['wavFileName'].isin(validation_filenames)]\n",
    "\n",
    "    # do the same for the noise\n",
    "    training_noise_table = noise_table[noise_table['wavFileName'].isin(training_filenames)]\n",
    "    testing_noise_table = noise_table[noise_table['wavFileName'].isin(testing_filenames)]\n",
    "    #validation_noise_table = noise_table[noise_table['wavFileName'].isin(validation_filenames)]\n",
    "\n",
    "    # Compile test data into a format that the data generator can use\n",
    "    testing_label_dict = dict()\n",
    "    for label in call_types: \n",
    "        testing_label_dict[label] = testing_label_table.loc[testing_label_table[label] == True, [\"Label\", \"Start\", \"Duration\",\"End\",\"wav_path\",\"label_path\"]]\n",
    "    testing_label_dict[label_for_noise] = testing_noise_table[[\"Label\", \"Start\", \"Duration\",\"End\",\"wav_path\",\"label_path\"]]\n",
    "\n",
    "    # Compile training data into a format that the data generator can use\n",
    "    training_label_full_dict = dict()\n",
    "    '''training_label_dict = dict()\n",
    "    validation_label_dict = dict()\n",
    "    training_label_table = pd.DataFrame()\n",
    "    validation_label_table = pd.DataFrame()'''\n",
    "    for label in call_types: \n",
    "        training_label_full_dict[label] = training_label_full_table.loc[training_label_full_table[label] == True, [\"Label\", \"Start\", \"Duration\",\"End\",\"wav_path\",\"label_path\"]]\n",
    "    training_label_full_dict[label_for_noise] = training_noise_table[[\"Label\", \"Start\", \"Duration\",\"End\",\"wav_path\",\"label_path\"]]\n",
    "\n",
    "    #to deal with small datasets, the call_types that are not available for training need to be removed.\n",
    "    #call = training_label_full_dict.keys()\n",
    "    for call in list(training_label_full_dict):\n",
    "        if training_label_full_dict[call].shape[0] < 2:\n",
    "            del training_label_full_dict[call]\n",
    "            del testing_label_dict[call]\n",
    "\n",
    "    # subset validation from training dict\n",
    "    training_label_dict = dict()\n",
    "    validation_label_dict = dict()\n",
    "    for label in training_label_full_dict:        \n",
    "        # reshuffle rows\n",
    "        #training_label_full_dict[label] = training_label_full_dict[label].sample(frac=1)#.reset_index(drop=True)\n",
    "        #subset by training and test set        \n",
    "        split_index = floor(len(training_label_full_dict[label][\"Label\"]) * train_val_split )\n",
    "        training_label_dict[label] = training_label_full_dict[label].iloc[:split_index]#.reset_index(drop = True)\n",
    "        validation_label_dict[label] = training_label_full_dict[label].iloc[split_index:]#.reset_index(drop = True)\n",
    "\n",
    "    shuffle(training_files)\n",
    "    training_filenames = training_files#[:split_index]\n",
    "\n",
    "\n",
    "    # Compile validation data into a format that the data generator can use\n",
    "    #validation_label_dict = dict()\n",
    "    #for label in call_types: \n",
    "    #    validation_label_dict[label] = validation_label_table.loc[validation_label_table[label] == True, [\"Label\", \"Start\", \"Duration\",\"End\",\"wav_path\",\"label_path\"]]\n",
    "    #validation_label_dict[label_for_noise] = validation_label_table[[\"Label\", \"Start\", \"Duration\",\"End\",\"wav_path\",\"label_path\"]]\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    # TRAINING\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    ## Build the training and validation data generators (for real this time)\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    if is_forked == True:\n",
    "        # initiate the data generator\n",
    "        train_generator = bg.ForkedDataGenerator(training_label_dict,\n",
    "                                                 #training_label_table, \n",
    "                                                 training_label_full_table,\n",
    "                                                 spec_window_size,\n",
    "                                                 n_mels, \n",
    "                                                 window, \n",
    "                                                 fft_win , \n",
    "                                                 fft_hop , \n",
    "                                                 normalise,\n",
    "                                                 label_for_noise,\n",
    "                                                 label_for_other,\n",
    "                                                 min_scaling_factor,\n",
    "                                                 max_scaling_factor,\n",
    "                                                 n_per_call,\n",
    "                                                 other_ignored_in_training,\n",
    "                                                 mask_value,\n",
    "                                                 mask_vector)\n",
    "\n",
    "        # get a batch to estimate rnn parameters\n",
    "        x_train, y_train = train_generator.__next__()#__getitem__(0)\n",
    "\n",
    "        # initial parameters\n",
    "        num_calltypes = y_train[0].shape[2]\n",
    "        gru_units = y_train[0].shape[1] \n",
    "\n",
    "\n",
    "        # initialise the training data generator and validation data generator\n",
    "        train_generator = bg.ForkedDataGenerator(training_label_dict,\n",
    "                                                 #training_label_table, \n",
    "                                                 training_label_full_table,\n",
    "                                                 spec_window_size,\n",
    "                                                 n_mels, \n",
    "                                                 window, \n",
    "                                                 fft_win , \n",
    "                                                 fft_hop , \n",
    "                                                 normalise,\n",
    "                                                 label_for_noise,\n",
    "                                                 label_for_other,\n",
    "                                                 min_scaling_factor,\n",
    "                                                 max_scaling_factor,\n",
    "                                                 n_per_call,\n",
    "                                                 other_ignored_in_training,\n",
    "                                                 mask_value,\n",
    "                                                 mask_vector)\n",
    "\n",
    "        val_generator = bg.ForkedDataGenerator(validation_label_dict,\n",
    "                                               #validation_label_table, \n",
    "                                               training_label_full_table,\n",
    "                                               spec_window_size,\n",
    "                                               n_mels, \n",
    "                                               window, \n",
    "                                               fft_win , \n",
    "                                               fft_hop , \n",
    "                                               normalise,\n",
    "                                               label_for_noise,\n",
    "                                               label_for_other,\n",
    "                                               min_scaling_factor,\n",
    "                                               max_scaling_factor,\n",
    "                                               n_per_call,\n",
    "                                               other_ignored_in_training,\n",
    "                                               mask_value,\n",
    "                                               mask_vector)\n",
    "\n",
    "    else:\n",
    "        train_generator = bg.DataGenerator(training_label_dict,\n",
    "                                                 #training_label_table, \n",
    "                                                 training_label_full_table,\n",
    "                                                 spec_window_size,\n",
    "                                                 n_mels, \n",
    "                                                 window, \n",
    "                                                 fft_win , \n",
    "                                                 fft_hop , \n",
    "                                                 normalise,\n",
    "                                                 label_for_noise,\n",
    "                                                 label_for_other,\n",
    "                                                 min_scaling_factor,\n",
    "                                                 max_scaling_factor,\n",
    "                                                 n_per_call,\n",
    "                                                 other_ignored_in_training,\n",
    "                                                 mask_value,\n",
    "                                                 mask_vector)\n",
    "        x_train, y_train = train_generator.__next__()#__getitem__(0)\n",
    "\n",
    "        # initial parameters\n",
    "        num_calltypes = y_train[0].shape[2]\n",
    "        gru_units = y_train[0].shape[1] \n",
    "\n",
    "        train_generator = bg.DataGenerator(training_label_dict,\n",
    "                                                 #training_label_table, \n",
    "                                                 training_label_full_table,\n",
    "                                                 spec_window_size,\n",
    "                                                 n_mels, \n",
    "                                                 window, \n",
    "                                                 fft_win , \n",
    "                                                 fft_hop , \n",
    "                                                 normalise,\n",
    "                                                 label_for_noise,\n",
    "                                                 label_for_other,\n",
    "                                                 min_scaling_factor,\n",
    "                                                 max_scaling_factor,\n",
    "                                                 n_per_call,\n",
    "                                                 other_ignored_in_training,\n",
    "                                                 mask_value,\n",
    "                                                 mask_vector)\n",
    "\n",
    "        val_generator = bg.DataGenerator(validation_label_dict,\n",
    "                                         training_label_full_table,      \n",
    "                                         #validation_label_table, \n",
    "                                               spec_window_size,\n",
    "                                               n_mels, \n",
    "                                               window, \n",
    "                                               fft_win , \n",
    "                                               fft_hop , \n",
    "                                               normalise,\n",
    "                                               label_for_noise,\n",
    "                                               label_for_other,\n",
    "                                               min_scaling_factor,\n",
    "                                               max_scaling_factor,\n",
    "                                               n_per_call,\n",
    "                                               other_ignored_in_training,\n",
    "                                               mask_value,\n",
    "                                               mask_vector)    \n",
    "\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    ## CONSTRUCT THE RNN\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # initialise the model class\n",
    "    model = rnn.BuildNetwork(x_train, num_calltypes, filters, gru_units, dense_neurons, dropout, mask_value)\n",
    "\n",
    "    # build the model\n",
    "    if is_forked == True:\n",
    "        RNN_model = model.build_forked_masked_rnn()\n",
    "\n",
    "        # Multiple target losses\n",
    "        losses = {\n",
    "            'output_calltype' : 'categorical_crossentropy',  # classification\n",
    "            'output_callpresence' : 'mse'  # regression 0-1\n",
    "        }\n",
    "\n",
    "        loss_weights = {\n",
    "            'output_calltype' : 1.0,\n",
    "            'output_callpresence' : 0.05  # Making this too large is not a good idea as we really want the network paying attention to class\n",
    "        }\n",
    "        accuracy_metrics = {\n",
    "            'output_calltype' : 'categorical_accuracy',\n",
    "            'output_callpresence' : 'binary_accuracy'\n",
    "\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        RNN_model = model.build_masked_rnn()\n",
    "        # only one target loss\n",
    "        losses = 'categorical_crossentropy'\n",
    "        loss_weights = None\n",
    "        accuracy_metrics = 'categorical_accuracy'\n",
    "\n",
    "    # Adam optimiser\n",
    "    adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "    # Compile the model\n",
    "    #RNN_model.compile(optimizer=adam, loss=losses, loss_weights=loss_weights, metrics=['binary_accuracy'])\n",
    "    RNN_model.compile(optimizer=adam, loss=losses, loss_weights=loss_weights, metrics = accuracy_metrics)\n",
    "\n",
    "    # Setup callbycks: learning rate / loss /tensorboard\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_plat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=25, verbose=1,\n",
    "                                       mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.000001)\n",
    "    loss = LossHistory()\n",
    "\n",
    "    # setup a path with a timestamp\n",
    "    date_time = datetime.datetime.now()\n",
    "    date_now = str(date_time.date())\n",
    "    time_now = str(date_time.time())\n",
    "    save_tensorboard_path = os.path.join(save_model_path, \"tensorboard_logs_\" + date_now + \"_\" + time_now)\n",
    "    if not os.path.isdir(save_tensorboard_path):\n",
    "        os.makedirs(save_tensorboard_path)   \n",
    "\n",
    "    # tensorboard\n",
    "    tensorboard = TensorBoard(log_dir = save_tensorboard_path,\n",
    "                              histogram_freq=0,\n",
    "                              write_graph=True,  # Show the network\n",
    "                              write_grads=True   # Show gradients\n",
    "                              )  \n",
    "    # fit model\n",
    "    RNN_model.fit_generator(train_generator, \n",
    "                            steps_per_epoch = train_generator.__len__(),\n",
    "                            epochs = epochs,\n",
    "                            callbacks = [early_stopping, reduce_lr_plat, loss, tensorboard],\n",
    "                            validation_data = val_generator,\n",
    "                            validation_steps = val_generator.__len__())\n",
    "\n",
    "\n",
    "    # Look at the model\n",
    "    # plot_model(RNN_model, run_name + \".png\", show_shapes=True)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    ## SAVE THE MODEL\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # save the model\n",
    "    date_time = datetime.datetime.now()\n",
    "    date_now = str(date_time.date())\n",
    "    time_now = str(date_time.time())\n",
    "    sf = os.path.join(save_model_path, run_name+ \"_\" + date_now + \"_\" + time_now)\n",
    "    if not os.path.isdir(sf):\n",
    "        os.makedirs(sf)\n",
    "\n",
    "    RNN_model.save(sf + '/savedmodel' + '.h5')\n",
    "\n",
    "    print(sf)\n",
    "\n",
    "    test = load_model(sf + '/savedmodel' + '.h5')\n",
    "    test.summary()\n",
    "    #test.predict([spec])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    ## TESTING\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    ## Predict over the test files\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    skipped_files = []\n",
    "\n",
    "    # we only do one file in this example to avoid it running too long\n",
    "    for file_ID in testing_filenames:\n",
    "        # file_ID = testing_filenames[0] # for testing purposes only\n",
    "\n",
    "        # subset the label table to only use that one file\n",
    "        file_label_table = pd.DataFrame(testing_label_table[testing_label_table['wavFileName'].isin([file_ID])])\n",
    "\n",
    "        #sometimes the labels are missing and categorise those as \"oth\"\n",
    "        file_label_table['Label'] = file_label_table['Label'].replace(np.nan, \"oth\")\n",
    "\n",
    "        file_label_table = file_label_table.sort_values(by=['Start']).reset_index()\n",
    "        # Find the file ID name i.e. remove the extention\n",
    "        file_ID = file_ID.split(\".\")[0] \n",
    "        # find the matching audio for the label data\n",
    "        audio_path = file_label_table[\"wav_path\"][0]   \n",
    "\n",
    "        print(\"*****************************************************************\")   \n",
    "        print(\"*****************************************************************\") \n",
    "        print (\"File being processed : \" + audio_path)    \n",
    "\n",
    "        # find the start and stop  of the labelling periods (also using skipon/skipoff)\n",
    "        loop_table = file_label_table.loc[file_label_table[\"Label\"].str.contains('|'.join(label_for_startstop), regex=True, case = False), [\"Label\",\"Start\"]]\n",
    "        loop_times = list(loop_table[\"Start\"])\n",
    "\n",
    "        # Make sure that the file contains the right number of start and stops, otherwise go to the next file\n",
    "        if len(loop_times)%2 != 0:\n",
    "            print(\"!!!!!!!!!!!!!!!!\")\n",
    "            warnings.warn(\"There is a missing start or stop in this file and it has been skipped: \" + audio_path)\n",
    "            skipped_files.append(file_ID)\n",
    "            continue \n",
    "\n",
    "        if len(loop_times) == 0:\n",
    "            print(\"!!!!!!!!!!!!!!!!\")\n",
    "            warnings.warn(\"There is a missing start or stop in this file and it has been skipped: \" + audio_path)\n",
    "            skipped_files.append(file_ID)\n",
    "            continue \n",
    "\n",
    "        # save the label_table\n",
    "        save_label_table_filename = file_ID + \"_LABEL_TABLE.txt\"\n",
    "        file_label_table.to_csv(os.path.join(save_label_table_test_path, save_label_table_filename), \n",
    "                            header=True, index=None, sep=';')\n",
    "\n",
    "        # load the audio data\n",
    "        y, sr = librosa.load(audio_path, sr=None, mono=False)\n",
    "\n",
    "        # # Reshaping the Audio file (mono) to deal with all wav files similarly\n",
    "        # if y.ndim == 1:\n",
    "        #     y = y.reshape(1, -1)\n",
    "\n",
    "        # # Implement this for acc data\n",
    "        # for ch in range(y.shape[0]):\n",
    "        # ch=0\n",
    "        # y_sub = y[:,ch]\n",
    "        y_sub = y\n",
    "\n",
    "        # loop through every labelling start based on skipon/off within this loop_table\n",
    "        for loopi in range(0, int(len(loop_times)), 2):\n",
    "            # loopi = 0\n",
    "            fromi =  loop_times[loopi]\n",
    "            #toi = fromi + 5\n",
    "            toi = loop_times[int(loopi + 1)] # define the end of the labelling periods\n",
    "\n",
    "            calltype_pred_list = []\n",
    "            callpresence_pred_list = []\n",
    "            # if the file exists, load it\n",
    "            if os.path.exists(os.path.join(save_pred_stack_test_path, file_ID + '_CALLTYPE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy')): \n",
    "                calltype_pred_list = np.load( os.path.join(save_pred_stack_test_path, file_ID + '_CALLTYPE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy')) \n",
    "                callpresence_pred_list = np.load( os.path.join(save_pred_stack_test_path, file_ID + '_CALLPRESENCE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy'))\n",
    "\n",
    "\n",
    "            # if the file is incomplete or non existant redo it\n",
    "            if (len(np.arange(fromi, toi, slide)) - sum(np.arange(fromi, toi, slide)+spec_window_size > toi)) != (len(calltype_pred_list)):\n",
    "\n",
    "                for spectro_slide in np.arange(fromi, toi, slide):                \n",
    "                    # spectro_slide = fromi\n",
    "                    start = round(spectro_slide,3)\n",
    "                    stop = round(spectro_slide + spec_window_size, 3)\n",
    "\n",
    "                    # ignore cases where the window is larger than what is labelled (e.g. at the end)\n",
    "                    if stop <= toi:\n",
    "\n",
    "                        #generate the spectrogram\n",
    "                        spectro = pre.generate_mel_spectrogram(y=y_sub, sr=sr, start=start, stop=stop, \n",
    "                                                                n_mels = n_mels, window='hann', \n",
    "                                                                fft_win= fft_win, fft_hop = fft_hop, \n",
    "                                                                normalise = True)\n",
    "\n",
    "                        # transpose it and put it in a format that works with the NN\n",
    "                        spec = spectro.T\n",
    "                        spec = spec[np.newaxis, ..., np.newaxis]  \n",
    "\n",
    "                        # generate a mask (as a placeholder) but don't mask anything as we are predicting and want to include other\n",
    "                        mask = np.asarray([True for i in range(spectro.shape[1])])\n",
    "                        mask = mask[np.newaxis,...]\n",
    "\n",
    "                        # generate the prediction\n",
    "                        pred = RNN_model.predict([spec,mask])\n",
    "\n",
    "                        # add this prediction to the stack that will be used to generate the predictions table\n",
    "                        if is_forked:\n",
    "                            calltype_pred_list.append(np.squeeze(pred[0]))\n",
    "                            callpresence_pred_list.append(np.squeeze(pred[1]))\n",
    "                        else:\n",
    "                            calltype_pred_list.append(np.squeeze(pred))\n",
    "\n",
    "                # save the prediction stacks\n",
    "                np.save( os.path.join(save_pred_stack_test_path, file_ID + '_CALLTYPE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy'), calltype_pred_list)\n",
    "                with open(os.path.join(save_pred_stack_test_path, file_ID + '_CALLTYPE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.txt'), \"w\") as f:\n",
    "                    for row in calltype_pred_list:\n",
    "                        f.write(str(row) +\"\\n\")\n",
    "                if is_forked:      \n",
    "                    np.save( os.path.join(save_pred_stack_test_path, file_ID + '_CALLPRESENCE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.npy'), callpresence_pred_list)\n",
    "                    with open(os.path.join(save_pred_stack_test_path, file_ID + '_CALLPRESENCE_PRED_STACK_' + str(fromi) + '-' + str(toi) + '.txt'), \"w\") as f:\n",
    "                        for row in callpresence_pred_list:\n",
    "                            f.write(str(row) +\"\\n\")\n",
    "\n",
    "            # Loop through different sets of thresholds\n",
    "            for low_thr in [0.1]:\n",
    "                for high_thr in [0.5,0.6,0.7,0.8,0.9,0.95]: \n",
    "\n",
    "                    # make sure it doesnt generate a 0.00098982374957839486 type number\n",
    "                    low_thr = round(low_thr,2)                               \n",
    "                    high_thr = round(high_thr,2)\n",
    "\n",
    "                    # stop the loop if the low threshold is bigger than the high threshold\n",
    "                    if low_thr >= high_thr:\n",
    "                        continue\n",
    "\n",
    "                    save_pred_table_filename = file_ID + \"_CALLTYPE_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + \".txt\"\n",
    "\n",
    "                    # if the file exists, pass to the next iteration of the loop\n",
    "                    #if os.path.exists(os.path.join(save_pred_table_test_path, save_pred_table_filename)):\n",
    "                    #    continue\n",
    "\n",
    "                    print(\"*****************************************************************\") \n",
    "                    print (\"Low Threshold: \" + str(low_thr))    \n",
    "                    print (\"High Threshold: \" + str(high_thr))  \n",
    "\n",
    "                    #----------------------------------------------------------------------------\n",
    "                    # Compile the predictions for each on/off labelling chunk\n",
    "                    detections = ppm.merge_p(probabilities = calltype_pred_list, \n",
    "                                              labels=list(testing_label_dict.keys()),#list(call_types.keys()),\n",
    "                                              starttime = 0, \n",
    "                                              frameadv_s = fft_hop, \n",
    "                                              specadv_s = slide,\n",
    "                                              low_thr=low_thr, \n",
    "                                              high_thr=high_thr, \n",
    "                                              debug=1)\n",
    "\n",
    "                    #in case the dataset was just noise, still create an empty placeholder to merge\n",
    "                    if len(detections) == 0:  \n",
    "                        detections = pd.DataFrame(columns = ['category', 'start', 'end', 'scores'])\n",
    "\n",
    "                    # create an empty dataset\n",
    "                    pred_table = pd.DataFrame() \n",
    "\n",
    "                    #convert these detections to a predictions table                \n",
    "                    table = pd.DataFrame(detections)\n",
    "                    table[\"Label\"] = table[\"category\"]\n",
    "                    table[\"Start\"] = round(table[\"start\"]*fft_hop + fromi, 3) #table[\"start\"].apply(Decimal)*Decimal(fft_hop) + Decimal(fromi)\n",
    "                    table[\"Duration\"] = round( (table[\"end\"]-table[\"start\"])*fft_hop, 3) #(table[\"end\"].apply(Decimal)-table[\"start\"].apply(Decimal))*Decimal(fft_hop)\n",
    "                    table[\"End\"] = round(table[\"end\"]*fft_hop + fromi, 3) #table[\"Start\"].apply(Decimal) + table[\"Duration\"].apply(Decimal)\n",
    "\n",
    "                    # keep only the useful columns    \n",
    "                    table = table[[\"Label\",\"Start\",\"Duration\", \"End\", \"scores\"]]  \n",
    "\n",
    "                    # Add a row which stores the start of the labelling period\n",
    "                    row_start = pd.DataFrame()\n",
    "                    row_start.loc[0,'Label'] = list(loop_table[\"Label\"])[loopi]\n",
    "                    row_start.loc[0,'Start'] = fromi\n",
    "                    row_start.loc[0,'Duration'] = 0\n",
    "                    row_start.loc[0,'End'] = fromi \n",
    "                    row_start.loc[0,'scores'] = [0] \n",
    "\n",
    "                    # Add a row which stores the end of the labelling period\n",
    "                    row_stop = pd.DataFrame()\n",
    "                    row_stop.loc[0,'Label'] = list(loop_table[\"Label\"])[int(loopi + 1)]\n",
    "                    row_stop.loc[0,'Start'] = toi\n",
    "                    row_stop.loc[0,'Duration'] = 0\n",
    "                    row_stop.loc[0,'End'] = toi \n",
    "                    row_stop.loc[0,'scores'] = [0]       \n",
    "\n",
    "                    # add the true false columns based on the call types dictionary\n",
    "                    for true_label in call_types:\n",
    "                        table[true_label] = False\n",
    "                        row_start[true_label] = False\n",
    "                        row_stop[true_label] = False\n",
    "                        for old_label in call_types[true_label]:\n",
    "                            table.loc[table[\"Label\"].str.contains(old_label, regex=True, case = False), true_label] = True\n",
    "\n",
    "                    # make sure start and stop are not classified as calls\n",
    "                    row_start[label_for_noise] = True\n",
    "                    row_stop[label_for_noise] = True\n",
    "\n",
    "                    # put these rows to the label table\n",
    "                    table = pd.concat([row_start, table, row_stop]) \n",
    "\n",
    "                    # add this table to the overall predictions table for that collar\n",
    "                    pred_table = pd.concat([pred_table, table ])\n",
    "\n",
    "                    # for each on/off labelling chunk, we can save the prediction and append it to the previous chunk\n",
    "                    if loopi == 0:                    \n",
    "                        # for the first chunck keep the header, but not when appending later. Also, overwrite old runs\n",
    "                        pred_table.to_csv(os.path.join(save_pred_table_test_path, save_pred_table_filename), \n",
    "                                          header=True, index=None, sep=';', mode = 'w')\n",
    "                    else:\n",
    "                        pred_table.to_csv(os.path.join(save_pred_table_test_path, save_pred_table_filename), \n",
    "                                          header=None, index=None, sep=';', mode = 'a')\n",
    "\n",
    "    '''\n",
    "    # # load the saved file\n",
    "    # with open(os.path.join(save_pred_stack_test_path, file_ID + '_PRED_STACK.txt')) as f:\n",
    "    #     content = f.readlines()\n",
    "    # # remove whitespace characters like `\\n` at the end of each line\n",
    "    # pred_list = [x.strip() for x in content] \n",
    "\n",
    "\n",
    "    # #or\n",
    "    # pred_list = np.load( os.path.join(save_pred_stack_test_path, file_ID + '_PRED_STACK.npy'))\n",
    "    # '''\n",
    "\n",
    "    # save the files that were skipped\n",
    "    print(skipped_files)\n",
    "\n",
    "    # save a copy of the training and testing diles\n",
    "    with open(os.path.join(save_model_path, \"skipped_testing_files.txt\"), \"w\") as f:\n",
    "        for s in skipped_files:\n",
    "            f.write(str(s) +\"\\n\")\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    ## TESTING\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # because of new file format, need to only keep certain columns so that the evaluation metrics work with the label tables\n",
    "    column_names = [\"Label\",\"Start\",\"Duration\",\"End\"]\n",
    "    column_names.extend(list(call_types.keys()))  \n",
    "\n",
    "    file_ID_list = [file_ID.split(\".\")[0] for file_ID in testing_filenames if file_ID.split(\".\")[0] not in skipped_files]\n",
    "    labels_list =  [os.path.join(save_label_table_test_path,file_ID.split(\".\")[0]  + \"_LABEL_TABLE.txt\" ) for file_ID in file_ID_list]\n",
    "\n",
    "    # get rid of duplicates\n",
    "    for file in labels_list :\n",
    "        df = pd.read_csv(file, delimiter=';') \n",
    "        # df = df.drop_duplicates(keep=False)\n",
    "        df = df[column_names]\n",
    "        df.to_csv(file, header=True, index=None, sep=';', mode = 'w')\n",
    "\n",
    "    for low_thr in [0.1]:\n",
    "        for high_thr in [0.5,0.6,0.7,0.8,0.9,0.95]: \n",
    "\n",
    "            low_thr = round(low_thr,2)                               \n",
    "            high_thr = round(high_thr,2) \n",
    "            print(\"**********************************************************\")\n",
    "            print(\"Evaluating thresholds: \" + str(low_thr) + \"-\" + str(high_thr))\n",
    "            print(\"**********************************************************\")\n",
    "\n",
    "            if low_thr >= high_thr:\n",
    "                continue\n",
    "\n",
    "            pred_list = [os.path.join(save_pred_table_test_path,file_ID.split(\".\")[0]  + \"_CALLTYPE_PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + \".txt\" ) for file_ID in file_ID_list ]\n",
    "            evaluation = metrics.Evaluate(label_list = labels_list, \n",
    "                                          prediction_list = pred_list, \n",
    "                                          noise_label = \"noise\", \n",
    "                                          IoU_threshold = 0.2, \n",
    "                                          call_analysis = eval_analysis, \n",
    "                                          GT_proportion_cut = 0.01, \n",
    "                                          no_call = no_call,\n",
    "                                          headers = set(['Label', 'Duration', 'Start', 'End']),\n",
    "                                          nonfoc_tags =[\"NONFOC\", \"nf\", \"*\"]\n",
    "                                         ) # 0.99 is 0.5\n",
    "            output, skipped_calls = evaluation.main()        \n",
    "            print(str(skipped_calls) + \" calls were skipped in total\")\n",
    "\n",
    "            to_pickle = [\"Time_Difference\", \"Matching_Table\", \"Prediction_Indices\", \"Label_Indices\" , \"_Match\"]\n",
    "            for metric in output.keys():\n",
    "                if any(ext in metric for ext in to_pickle):\n",
    "                    filename = \"PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + \"_\" +str(metric) +\".p\"\n",
    "                    with open(os.path.join(save_metrics_path_eval, filename), 'wb') as fp:\n",
    "                        pickle.dump(output[metric], fp) \n",
    "                else:\n",
    "                    filename = \"PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + \"_\" +str(metric) +\".csv\"\n",
    "                    output[metric].to_csv(os.path.join(save_metrics_path_eval, filename))    \n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "    # PLOT A PREDICTION\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    pool = audiopool.AudioPool() \n",
    "\n",
    "    # Find a call\n",
    "    call = testing_label_dict[\"cc\"].iloc[0]\n",
    "    label_subset = testing_label_table[testing_label_table['wav_path'].isin([call[\"wav_path\"]])]\n",
    "\n",
    "    # set the labels\n",
    "    label_list = list(testing_label_dict.keys())#list(call_types.keys())\n",
    "    if other_ignored_in_training:\n",
    "        label_list.remove(label_for_other)\n",
    "\n",
    "\n",
    "    # randomise the start a little so the new spectrogram will be a little different from the old\n",
    "    # if the call is very long have a large range to draw the window\n",
    "    if call[\"Duration\"]>= spec_window_size:\n",
    "        call_start = round(float(np.random.uniform(call[\"Start\"]-spec_window_size/2, \n",
    "                                                   call[\"End\"]-spec_window_size/2, 1)), 3)\n",
    "    # if the call is short call, draw it from somewhere\n",
    "    else:\n",
    "        call_start = round(float(np.random.uniform((call[\"Start\"]+call[\"End\"])/2-spec_window_size, \n",
    "                                                   (call[\"Start\"]+call[\"End\"])/2)), 3)\n",
    "\n",
    "    # load in a subsection of the spectrogram\n",
    "    # y, sr = librosa.load(call[\"wav_path\"], sr=None, mono=False,\n",
    "    #                      offset = call_start, duration =self.spec_window_size)\n",
    "    y = pool.get_seconds(call[\"wav_path\"], call_start, spec_window_size)\n",
    "    sr = pool.get_Fs(call[\"wav_path\"])\n",
    "\n",
    "    call_stop = round(call_start + spec_window_size,3 )\n",
    "\n",
    "    # have it as an array\n",
    "    data_subset = np.asfortranarray(y)\n",
    "\n",
    "    #get the spectrogram\n",
    "    spectrogram = pre.generate_mel_spectrogram(data_subset, sr, 0, spec_window_size, \n",
    "                                               n_mels, window, fft_win , fft_hop , normalise)\n",
    "\n",
    "    # generate label\n",
    "    label = pre.create_label_matrix(label_subset, \n",
    "                                    spectrogram, \n",
    "                                    testing_label_dict, \n",
    "                                    call_start, \n",
    "                                    call_stop, \n",
    "                                    label_for_noise, \n",
    "                                    label_for_other, \n",
    "                                    other_ignored_in_training)\n",
    "    # plot spectrogram\n",
    "    plt.figure(figsize=(10,4))\n",
    "    #plt.subplot(411)\n",
    "    yaxis = range(0, np.flipud(spectrogram).shape[0]+1)\n",
    "    xaxis = range(0, np.flipud(spectrogram).shape[1]+1)\n",
    "    plt.xticks(np.arange(0, np.flipud(label).shape[1]+1,50),\n",
    "               list(label.columns[np.arange(0, np.flipud(label).shape[1]+1,50)]))\n",
    "    librosa.display.specshow(spectrogram,  y_axis='mel', x_coords = label.columns)#, x_axis= \"time\",sr=sr, x_coords = label.columns)\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.clim(-35, 35)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.show()\n",
    "\n",
    "    # plot LABEL\n",
    "    #plt.subplot(412)\n",
    "    #print(label.shape)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    xaxis = range(0, np.flipud(label).shape[1]+1)\n",
    "    yaxis = range(0, np.flipud(label).shape[0]+1)\n",
    "    plt.yticks(np.arange(0.5, len(label_list)+0.5 ,1 ),reversed(label_list))\n",
    "    plt.xticks(np.arange(0, np.flipud(label).shape[1]+1,50),\n",
    "               list(label.columns[np.arange(0, np.flipud(label).shape[1]+1,50)]))\n",
    "    plt.pcolormesh(xaxis, yaxis, np.flipud(label))\n",
    "    plt.clim(0, 1)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Calltype')\n",
    "    plt.colorbar(label=\"Label\")\n",
    "    plt.show()\n",
    "\n",
    "    if is_forked:\n",
    "        # generate matrix of call/not call\n",
    "        callmat = pre.create_call_matrix(label_subset, spectrogram, call_start, call_stop, \n",
    "                                          label_for_noise, label_for_other, other_ignored_in_training)\n",
    "\n",
    "    # prediction\n",
    "    # transpose it and put it in a format that works with the NN\n",
    "    spec = spectrogram.T\n",
    "    spec = spec[np.newaxis, ..., np.newaxis]  \n",
    "\n",
    "    # generate a mask (as a placeholder) but don't mask anything as we are predicting and want to include other\n",
    "    mask = np.asarray([True for i in range(spectrogram.shape[1])])\n",
    "    mask = mask[np.newaxis,...]\n",
    "\n",
    "    # generate the prediction\n",
    "    pred = RNN_model.predict([spec,mask])\n",
    "\n",
    "    # add this prediction to the stack that will be used to generate the predictions table\n",
    "    if is_forked:\n",
    "        calltype_pred= pred[0]\n",
    "        callpresence_pred = pred[1]   \n",
    "    else:\n",
    "        calltype_pred = pred\n",
    "\n",
    "    pred = calltype_pred[0].T\n",
    "\n",
    "    #print(pred.shape)\n",
    "    #plot calltype prediction\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    xaxis = range(0, np.flipud(label).shape[1]+1)\n",
    "    yaxis = range(0, np.flipud(label).shape[0]+1)\n",
    "    plt.yticks(np.arange(0.5, len(label_list)+0.5 ,1 ),reversed(label_list))\n",
    "    plt.xticks(np.arange(0, np.flipud(label).shape[1]+1,50),\n",
    "               list(label.columns[np.arange(0, np.flipud(label).shape[1]+1,50)]))\n",
    "    plt.pcolormesh(xaxis, yaxis, np.flipud(pred))\n",
    "    plt.clim(0, 1)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Calltype Prediction')\n",
    "    plt.colorbar(label=\"Label\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    if is_forked:  \n",
    "        # plot call matrix\n",
    "        #plt.subplot(413)\n",
    "        plt.figure(figsize=(10, 1))\n",
    "        xaxis = range(0, np.flipud(label).shape[1]+1)\n",
    "        yaxis = range(0, np.flipud(callmat).shape[0]+1)\n",
    "        plt.yticks(np.arange(0.5, callmat.shape[0]+0.5 ,1 ), reversed(callmat.index.values))\n",
    "        plt.xticks(np.arange(0, np.flipud(label).shape[1]+1,50),\n",
    "                   list(label.columns[np.arange(0, np.flipud(label).shape[1]+1,50)]))\n",
    "        plt.pcolormesh(xaxis, yaxis, np.flipud(callmat))\n",
    "        plt.clim(0, 1)\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Call / No Call')\n",
    "        plt.colorbar(label=\"Label\")\n",
    "        plt.show()\n",
    "\n",
    "        pred = callpresence_pred[0].T\n",
    "        #plot prediction\n",
    "        plt.figure(figsize=(10, 1))\n",
    "        xaxis = range(0, np.flipud(label).shape[1]+1)\n",
    "        yaxis = range(0, np.flipud(callmat).shape[0]+1)\n",
    "        plt.yticks(np.arange(0.5, callmat.shape[0]+0.5 ,1 ), reversed(callmat.index.values))\n",
    "        plt.xticks(np.arange(0, np.flipud(label).shape[1]+1,50),\n",
    "                   list(label.columns[np.arange(0, np.flipud(label).shape[1]+1,50)]))\n",
    "        plt.pcolormesh(xaxis, yaxis, np.flipud(pred))\n",
    "        plt.clim(0, 1)\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Call / No Call prediction')\n",
    "        plt.colorbar(label=\"Label\")\n",
    "        plt.show()\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "    # CONFUSION MATRIX\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "\n",
    "    # loop over the threaholdsf\n",
    "    for low_thr in [0.1]:\n",
    "        for high_thr in [0.5,0.6,0.7,0.8,0.9,0.95]:     \n",
    "            #########################################\n",
    "            # FORMAT \n",
    "\n",
    "            low_thr = round(low_thr,2)                               \n",
    "            high_thr = round(high_thr,2) \n",
    "\n",
    "            if low_thr >= high_thr:\n",
    "                continue\n",
    "            if low_thr == 0.1 and high_thr == 0.2:\n",
    "                continue\n",
    "\n",
    "            if eval_analysis == \"normal\":\n",
    "                confusion_filename = os.path.join(save_metrics_path_eval, \"PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + '_foc_Confusion_Matrix.csv')\n",
    "            else:\n",
    "                confusion_filename = os.path.join(save_metrics_path_eval, \"PRED_TABLE_thr_\" + str(low_thr) + \"-\" + str(high_thr) + '__Confusion_Matrix.csv')\n",
    "            with open(confusion_filename, newline='') as csvfile:\n",
    "                array = list(csv.reader(csvfile))\n",
    "\n",
    "            df_cm = pd.DataFrame(array) #, range(6), range(6))    \n",
    "\n",
    "            # get rid of the weird indentations and make rows and columns as names\n",
    "            new_col = df_cm.iloc[0] # grab the first row for the header\n",
    "            df_cm = df_cm[1:] # take the data less the header row\n",
    "            df_cm.columns = new_col # set the header row as the df header    \n",
    "            new_row = df_cm['']\n",
    "            df_cm = df_cm.drop('', 1)\n",
    "            df_cm.index = new_row\n",
    "            df_cm.index.name= None\n",
    "            df_cm.columns.name= None\n",
    "\n",
    "            # # replace FP and FN with noise\n",
    "            df_cm['noise'] = df_cm['FN'] \n",
    "            #df_cm.loc['noise']=df_cm.loc['FP']\n",
    "\n",
    "            # remove FP and FN\n",
    "            df_cm = df_cm.drop(\"FN\", axis=1)\n",
    "            #df_cm = df_cm.drop(\"FP\", axis=0)\n",
    "\n",
    "            df_cm = df_cm.apply(pd.to_numeric)\n",
    "\n",
    "            # Raw confusion matrix\n",
    "            df_cm = df_cm[list(testing_label_dict.keys())]\n",
    "            df_cm = df_cm.reindex(list(testing_label_dict.keys()))       \n",
    "\n",
    "            #########################################\n",
    "            # CALCULATE\n",
    "\n",
    "            # Recall confusion matrix\n",
    "            df_recall = df_cm.div(df_cm.sum(axis=1), axis=0).round(2)#pd.DataFrame(df_cm.values / df_cm.sum(axis=1).values).round(2)\n",
    "\n",
    "            # Proportion of calls for confusion matrix\n",
    "            call_len = list()\n",
    "            for i in testing_label_dict.keys():\n",
    "                call_len.append(testing_label_dict[i].shape[0])\n",
    "            # add noise at the end\n",
    "            call_len[-1] = df_cm.sum(axis=1)[-1]\n",
    "\n",
    "            #proportion of calls\n",
    "            df_prop = df_cm.div(call_len, axis=0).round(2)#pd.DataFrame(df_cm.values / df_cm.sum(axis=1).values).round(2)\n",
    "\n",
    "            #########################################\n",
    "            # PLOT\n",
    "\n",
    "            #multi figure parameters\n",
    "            fig,((ax1,ax2,ax3)) = plt.subplots(1,3, figsize=(20,5))\n",
    "            fig.suptitle(str(low_thr) + \" - \" + str(high_thr))\n",
    "\n",
    "            # plot raw\n",
    "            sn.set(font_scale=1.1) # for label size\n",
    "            sn.heatmap((df_cm+1), annot=df_cm, fmt='g',norm = LogNorm(), annot_kws={\"size\": 10}, ax= ax1) # font size\n",
    "            ax1.set_title(\"Raw\")              \n",
    "\n",
    "            # plot recall\n",
    "            sn.set(font_scale=1.1) # for label size\n",
    "            sn.heatmap((df_recall), annot=True, fmt='g', annot_kws={\"size\": 10}, ax= ax2) # font size\n",
    "            ax2.set_title(\"Recall\" )\n",
    "\n",
    "            # plot proportion of calls\n",
    "            sn.set(font_scale=1.1) # for label size\n",
    "            sn.heatmap((df_prop), annot=True, fmt='g', annot_kws={\"size\": 10}, ax= ax3) # font size\n",
    "            ax3.set_title(\"Call Prop\")\n",
    "\n",
    "            # Save 3 panels\n",
    "            plt.savefig(os.path.join(save_metrics_path, eval_analysis, \"Confusion_mat_thr_\" + str(low_thr) + \"-\" + str(high_thr) + '.png'))\n",
    "            plt.show()\n",
    "            plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML1_env",
   "language": "python",
   "name": "ml1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
